{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e965f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "import re\n",
    "from nltk.lm import KneserNeyInterpolated, Vocabulary\n",
    "import demoji \n",
    "from nltk.lm.smoothing import KneserNey\n",
    "import functools\n",
    "from nltk.util import everygrams\n",
    "from nltk.util import bigrams, ngrams\n",
    "from collections import Counter \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da5941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_url(query, start_time, end_time, max_results= 100):\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    query_params = {\"query\": f\"{query} lang:en\", \"expansions\": \"geo.place_id\",\n",
    "                    \"max_results\": max_results, \n",
    "                    \"place.fields\": \"country_code,full_name,geo\",\n",
    "                    \"tweet.fields\": \"author_id\",\n",
    "                   \"next_token\": {}}\n",
    "    return (url, query_params)\n",
    "    \n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code) + \"\\n\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1617741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keyword = \"nft\"\n",
    "max_results = 100\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "url, query_params = create_url(keyword, start_time, end_time, max_results)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "203074e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"csvFile = open(\"nfts.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['author id', 'tweet id', 'text', 'geo', 'country_code', 'full_name', 'bbox'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ae188a13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"count = 0\n",
    "json_response = connect_to_endpoint(url, headers, query_params)\n",
    "amnt = 0\n",
    "\n",
    "while(count < 10000):\n",
    "    count += amnt\n",
    "    for step, tweet in enumerate(json_response['data']):\n",
    "        author_id = tweet['author_id']\n",
    "        tweet_id = tweet['id'] \n",
    "        text = tweet['text'] \n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "        try: \n",
    "            country_code = json_response['includes']['places'][step]['country_code']\n",
    "        except:\n",
    "            country_code = \" \"\n",
    "        try: \n",
    "            full_name = json_response['includes']['places'][step]['full_name']\n",
    "        except:\n",
    "            full_name = \" \"\n",
    "        try: \n",
    "            bbox = json_response['includes']['places'][step]['geo']\n",
    "        except:\n",
    "            bbox = \" \"\n",
    "\n",
    "        res = [author_id, tweet_id, text, geo, country_code, full_name, bbox]\n",
    "        csvWriter.writerow(res)\n",
    "    \n",
    "    amnt = json_response['meta']['result_count']\n",
    "        \n",
    "    json_response = connect_to_endpoint(url, headers, query_params, \n",
    "                                        next_token = json_response['meta']['next_token'])\n",
    "    \n",
    "\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1fbe40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/25363498/is-there-a-standard-approach-to-returning-values-from-coroutine-endpoints\n",
    "class FINISH_PROCESSING_SIGNAL(Exception): pass\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "def tokenize_remove_emoticons(elem):\n",
    "    tokenized_list = tknzr.tokenize(elem)\n",
    "    # replacing emojis with their description, \n",
    "    # after tokenizer so as not to make them seperate words\n",
    "    return list(map(lambda x: demoji.replace_with_desc(x), tokenized_list))\n",
    "def apply_over(arr):\n",
    "    return np.array([[tokenize_remove_emoticons(x) for x in lst] for lst in arr], dtype = object)\n",
    "\n",
    "# NLTK is a crap library, and so I had to do this manually \n",
    "def flatten_2d_lst(lst = train):\n",
    "    train_flattened = []\n",
    "    for x in lst: \n",
    "        for j in x: \n",
    "            train_flattened.append(j)\n",
    "    return train_flattened\n",
    "\n",
    "def coroutine(func):\n",
    "    @functools.wraps(func)\n",
    "    def start(*args, **kwargs):\n",
    "        cr = func(*args, **kwargs)\n",
    "        next(cr)\n",
    "        return cr\n",
    "    return start\n",
    "\n",
    "def source(df, targets):\n",
    "    for target in targets: \n",
    "        target.send( df)\n",
    "        processed_data = target.throw(FINISH_PROCESSING_SIGNAL)\n",
    "        return processed_data\n",
    "    \n",
    "\"\"\"\n",
    "Note, these transformations are completely deterministic\n",
    "irrespective of the dataset being used, thus we can apply \n",
    "this initial part on the entire datafram.\"\"\"\n",
    "@coroutine  \n",
    "def clean_train_test_split(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            df = (yield)\n",
    "            # removing all ethereum addresses\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'0x[a-fA-F0-9]{40}', 'ethe_addy', x)) \n",
    "            \n",
    "            # replace all dollar amounts with vague \"dollar_amnt\" \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'\\$[0-9]+(.[0-9]+)?', 'dollar_amnt', x)) \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'\\d', 'random_num', x)) \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'(random_num)*random_num', 'random_num', x))\n",
    "            \n",
    "            #lowercasing all letters\n",
    "            df['text'] = df.text.str.lower()\n",
    "            # will keep the handles for the sake of n-gram approximation, but specifics don't matter\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"@([a-zA-Z0-9_]{1,50})\", 'randomhandle', x))\n",
    "            # removing url links\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "            # removing embedded links, videos\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "            train, test = train_test_split(df, test_size=1000, random_state=42)\n",
    "            for target in targets: \n",
    "                target.send( (train, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine  \n",
    "def train_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            trainer = PunktTrainer()\n",
    "            train, test = (yield)\n",
    "            array_strs =train[['text']].values.flatten()\n",
    "            array_strs_test =test[['text']].values.flatten()\n",
    "            map(lambda x: trainer.train(x), array_strs)\n",
    "            trainer.finalize_training()\n",
    "            tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "            for target in targets: \n",
    "                target.send( (tokenizer, array_strs, array_strs_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "            \n",
    "@coroutine  \n",
    "def segment_sentences(targets):\n",
    "    global temp1, temp2\n",
    "    try:\n",
    "        while True: \n",
    "            tokenizer, x, test = (yield)\n",
    "            segment_sentences = np.array([tokenizer.tokenize(xi) for xi in x], dtype = object)\n",
    "            segment_sentences_test = np.array([tokenizer.tokenize(xi) for xi in test], dtype = object)\n",
    "            temp1, temp2 = segment_sentences, segment_sentences_test\n",
    "            for target in targets: \n",
    "                target.send( (segment_sentences, segment_sentences_test))\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "\n",
    "@coroutine   \n",
    "def sentence_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            x, test = apply_over(x), apply_over(test)\n",
    "            \n",
    "           #print(new_x)\n",
    "            flattened, flattened_test = [], []\n",
    "            for xi in x:\n",
    "                flattened.extend(xi)\n",
    "            for xi in test:\n",
    "                flattened_test.extend(xi)\n",
    "            for target in targets: \n",
    "                target.send( (flattened, flattened_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine   \n",
    "def unk_placement(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            counter = Counter(flatten_2d_lst(x))\n",
    "            filtered_list = Counter({k: c for k, c in counter.items() if c >= 2})\n",
    "            filtered_set = set(filtered_list.keys())\n",
    "            \n",
    "            for step, xi in enumerate(x): \n",
    "                for step2, j in enumerate(xi): \n",
    "                    if j not in filtered_set: \n",
    "                        x[step][step2] = '<UNK>'\n",
    "            \n",
    "            for target in targets:\n",
    "                target.send( (x, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine\n",
    "def sink():\n",
    "    data = {}\n",
    "    try:\n",
    "        while True:\n",
    "            data = (yield)\n",
    "    except FINISH_PROCESSING_SIGNAL:\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c1eafd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sink()\n",
    "b = unk_placement(targets = [a])\n",
    "c = sentence_tokenizer(targets = [b])\n",
    "d = segment_sentences(targets = [c])\n",
    "e = train_tokenizer(targets = [d])\n",
    "f = clean_train_test_split(targets = [e])\n",
    "\n",
    "df = pd.read_csv(\"nfts.csv\")\n",
    "\n",
    "train, test = source(df, targets = [f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "76f11769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_padding(n, dataset = train): \n",
    "    train_copy = copy.deepcopy(dataset)\n",
    "    for step, x in enumerate(train_copy): \n",
    "        train_copy[step] = list(pad_both_ends(x, n=n))\n",
    "        train_copy[step] = list(ngrams(train_copy[step], n=n))\n",
    "    return train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "07747bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gen_new_model(n = 1, test_set = test):\n",
    "    global train_vocab\n",
    "    train_copy = gen_new_padding(n)\n",
    "    test_copy = gen_new_padding(n, dataset = test)\n",
    "    train_vocab = Counter(flatten_2d_lst(train_copy))\n",
    "    model = KneserNeyInterpolated(n, discount=0.1)\n",
    "    model.fit(train_copy , train_vocab)\n",
    "    return train_copy, test_copy, model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2a5f6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_frequency(word):\n",
    "    if word in model.vocab.counts: \n",
    "        count = model.vocab.counts[word]\n",
    "    else:\n",
    "        count = model.vocab.counts['<UNK>',]\n",
    "    return np.log2(count/model.counts.N())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5a8c5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram perplexity score of any test set tweet is : 670.2454773666299 \n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "for j in test_set:\n",
    "    avg = np.mean([get_log_frequency(i) for i in j])\n",
    "    perplexity = np.power(2, -avg)\n",
    "    perplexities.append(perplexity)\n",
    "    \n",
    "perplexities_avg = np.mean(perplexities)\n",
    "print(f\"The average unigram perplexity score of any test set tweet is : {perplexities_avg} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b43859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
