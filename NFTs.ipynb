{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e965f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import re\n",
    "from nltk.lm import MLE\n",
    "import demoji \n",
    "from nltk.lm import Vocabulary\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da5941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_url(query, start_time, end_time, max_results= 100):\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    query_params = {\"query\": f\"{query} lang:en\", \"expansions\": \"geo.place_id\",\n",
    "                    \"max_results\": max_results, \n",
    "                    \"place.fields\": \"country_code,full_name,geo\",\n",
    "                    \"tweet.fields\": \"author_id\",\n",
    "                   \"next_token\": {}}\n",
    "    return (url, query_params)\n",
    "    \n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code) + \"\\n\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1617741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keyword = \"nft\"\n",
    "max_results = 100\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "url, query_params = create_url(keyword, start_time, end_time, max_results)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "203074e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"csvFile = open(\"nfts.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['author id', 'tweet id', 'text', 'geo', 'country_code', 'full_name', 'bbox'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ae188a13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"count = 0\n",
    "json_response = connect_to_endpoint(url, headers, query_params)\n",
    "amnt = 0\n",
    "\n",
    "while(count < 10000):\n",
    "    count += amnt\n",
    "    for step, tweet in enumerate(json_response['data']):\n",
    "        author_id = tweet['author_id']\n",
    "        tweet_id = tweet['id'] \n",
    "        text = tweet['text'] \n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "        try: \n",
    "            country_code = json_response['includes']['places'][step]['country_code']\n",
    "        except:\n",
    "            country_code = \" \"\n",
    "        try: \n",
    "            full_name = json_response['includes']['places'][step]['full_name']\n",
    "        except:\n",
    "            full_name = \" \"\n",
    "        try: \n",
    "            bbox = json_response['includes']['places'][step]['geo']\n",
    "        except:\n",
    "            bbox = \" \"\n",
    "\n",
    "        res = [author_id, tweet_id, text, geo, country_code, full_name, bbox]\n",
    "        csvWriter.writerow(res)\n",
    "    \n",
    "    amnt = json_response['meta']['result_count']\n",
    "        \n",
    "    json_response = connect_to_endpoint(url, headers, query_params, \n",
    "                                        next_token = json_response['meta']['next_token'])\n",
    "    \n",
    "\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b74e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"nfts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857e7da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author id</th>\n",
       "      <th>tweet id</th>\n",
       "      <th>text</th>\n",
       "      <th>geo</th>\n",
       "      <th>country_code</th>\n",
       "      <th>full_name</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356311832542248963</td>\n",
       "      <td>1495290075822936064</td>\n",
       "      <td>RT @NFTrepzy: 🌸👹WL GIVEAWAY!👹🌸\\n\\n2 x @Sekira_...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704677183386615808</td>\n",
       "      <td>1495290075546284036</td>\n",
       "      <td>RT @CryptoKojima: This is the front view of th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1460155383041904642</td>\n",
       "      <td>1495290075462176770</td>\n",
       "      <td>@CptHodl 2022 is going to be the year everyone...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1495196838990807041</td>\n",
       "      <td>1495290075357364224</td>\n",
       "      <td>@Blockworks_ @MattWallace888 @meta_ruffy bull ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1261475170520203264</td>\n",
       "      <td>1495290075290431489</td>\n",
       "      <td>RT @cryptattoo: $50 💸 6 hours\\n\\nMust retweet ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author id             tweet id  \\\n",
       "0  1356311832542248963  1495290075822936064   \n",
       "1   704677183386615808  1495290075546284036   \n",
       "2  1460155383041904642  1495290075462176770   \n",
       "3  1495196838990807041  1495290075357364224   \n",
       "4  1261475170520203264  1495290075290431489   \n",
       "\n",
       "                                                text geo country_code  \\\n",
       "0  RT @NFTrepzy: 🌸👹WL GIVEAWAY!👹🌸\\n\\n2 x @Sekira_...                    \n",
       "1  RT @CryptoKojima: This is the front view of th...                    \n",
       "2  @CptHodl 2022 is going to be the year everyone...                    \n",
       "3  @Blockworks_ @MattWallace888 @meta_ruffy bull ...                    \n",
       "4  RT @cryptattoo: $50 💸 6 hours\\n\\nMust retweet ...                    \n",
       "\n",
       "  full_name bbox  \n",
       "0                 \n",
       "1                 \n",
       "2                 \n",
       "3                 \n",
       "4                 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9ca38",
   "metadata": {},
   "source": [
    "##### Note, these transformations are completely deterministic irrespective of the dataset being used, thus we can apply this initial part on the entire datafram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8321e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src: https://medium.com/analytics-vidhya/pre-processing-tweets-for-sentiment-analysis-a74deda9993e\n",
    "\n",
    "# remove ethereum addresses from tweets, will replace with ethe_addy for the sake of n-gram \n",
    "# regeneration\n",
    "df.text = df.text.apply(lambda x: re.sub(r'0x[a-fA-F0-9]{40}', 'ethe_addy', x)) \n",
    "\n",
    "#lowercasing all letters\n",
    "df['text'] = df.text.str.lower()\n",
    "\n",
    "# will keep the handles for the sake of n-gram approximation, but specifics don't matter\n",
    "df.text = df.text.apply(lambda x: re.sub(r\"@([a-zA-Z0-9_]{1,50})\", 'randomhandle', x))\n",
    "\n",
    "# removing url links\n",
    "df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "\n",
    "# removing embedded links, videos\n",
    "df.text = df.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "df.text = df.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17867af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbe40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "tokenizer = {}\n",
    "\n",
    "def tokenize_remove_emoticons(elem):\n",
    "    tokenized_list = tknzr.tokenize(elem)\n",
    "    # replacing emojis with their description, \n",
    "    # after tokenizer so as not to make them seperate words\n",
    "    return list(map(lambda x: demoji.replace_with_desc(x), tokenized_list))\n",
    "def tokenize_sentences(array_elem):\n",
    "    return list(map(lambda x: tokenize_remove_emoticons(x), array_elem))\n",
    "vf = np.vectorize(tokenize_sentences)\n",
    "\n",
    "def coroutine(func):\n",
    "    @functools.wraps(func)\n",
    "    def start(*args, **kwargs):\n",
    "        cr = func(*args, **kwargs)\n",
    "        next(cr)\n",
    "        return cr\n",
    "    return start\n",
    "\n",
    "def source(train, targets):\n",
    "    for target in targets: \n",
    "        target.send(train)\n",
    "              \n",
    "@coroutine\n",
    "def train_tokenizer(targets):\n",
    "    global tokenizer\n",
    "    while True: \n",
    "        trainer = PunktTrainer()\n",
    "        train = (yield)\n",
    "        array_strs =train[['text']].values.flatten()\n",
    "        #print(train)\n",
    "        map(lambda x: trainer.train(x), array_strs)\n",
    "        trainer.finalize_training()\n",
    "        tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "        for target in targets: \n",
    "            target.send( (tokenizer, array_strs) )\n",
    "\n",
    "@coroutine\n",
    "def segment_sentences(targets):\n",
    "    while True: \n",
    "        tokenizer, x = (yield)\n",
    "        segment_sentences = np.array([tokenizer.tokenize(xi) for xi in x], dtype = object)\n",
    "        for target in targets: \n",
    "            target.send(segment_sentences)\n",
    "\n",
    "@coroutine\n",
    "def tokenizer(targets):\n",
    "    while True: \n",
    "        x = (yield)\n",
    "        x = vf(x.flatten())\n",
    "        flattened = []\n",
    "        for xi in x:\n",
    "            flattened.extend(xi)\n",
    "        for target in targets: \n",
    "            target.send(flattened)\n",
    "        \n",
    "@coroutine\n",
    "def train_model(targets):\n",
    "    while True: \n",
    "        n = 3\n",
    "        data = (yield)\n",
    "        train, vocab = padded_everygram_pipeline(n, data)\n",
    "        model = MLE(n)\n",
    "        model.fit(train, vocab)\n",
    "        for target in targets: \n",
    "            target.send(model)\n",
    "    \n",
    "@coroutine        \n",
    "def sink():\n",
    "    while True: \n",
    "        yield (yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1eafd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2365: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  otypes = ''.join([asarray(outputs[_k]).dtype.char\n"
     ]
    }
   ],
   "source": [
    "model = source(train, targets = [train_tokenizer(targets = [\\\n",
    "                                       segment_sentences(targets = [\\\n",
    "                                                         tokenizer(targets = [\\\n",
    "                                                                   train_model(targets = [\\\n",
    "                                                                               sink()])])])])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
