{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8e965f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "import re\n",
    "from nltk.lm import KneserNeyInterpolated, Vocabulary\n",
    "import demoji \n",
    "from nltk.lm.smoothing import KneserNey\n",
    "import functools\n",
    "from nltk.util import everygrams\n",
    "from nltk.util import bigrams, ngrams\n",
    "from collections import Counter \n",
    "import copy\n",
    "from numba import njit\n",
    "from re import search\n",
    "import random \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9da5941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# src: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_url(query, max_results= 100):\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    query_params = {\"query\": f\"{query} lang:en\", \"expansions\": \"geo.place_id\",\n",
    "                    \"max_results\": max_results, \n",
    "                    \"place.fields\": \"country_code,full_name,geo\",\n",
    "                    \"tweet.fields\": \"author_id\",\n",
    "                   \"next_token\": {}}\n",
    "    return (url, query_params)\n",
    "    \n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code) + \"\\n\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1617741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keyword = \"football\"\n",
    "max_results = 100\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "url, query_params = create_url(keyword, max_results)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "203074e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"csvFile = open(\"football.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['author id', 'tweet id', 'text', 'geo', 'country_code', 'full_name', 'bbox'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae188a13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"count = 0\n",
    "json_response = connect_to_endpoint(url, headers, query_params)\n",
    "amnt = 0\n",
    "\n",
    "while(count < 1000):\n",
    "    count += amnt\n",
    "    for step, tweet in enumerate(json_response['data']):\n",
    "        author_id = tweet['author_id']\n",
    "        tweet_id = tweet['id'] \n",
    "        text = tweet['text'] \n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "        try: \n",
    "            country_code = json_response['includes']['places'][step]['country_code']\n",
    "        except:\n",
    "            country_code = \" \"\n",
    "        try: \n",
    "            full_name = json_response['includes']['places'][step]['full_name']\n",
    "        except:\n",
    "            full_name = \" \"\n",
    "        try: \n",
    "            bbox = json_response['includes']['places'][step]['geo']\n",
    "        except:\n",
    "            bbox = \" \"\n",
    "\n",
    "        res = [author_id, tweet_id, text, geo, country_code, full_name, bbox]\n",
    "        csvWriter.writerow(res)\n",
    "    \n",
    "    amnt = json_response['meta']['result_count']\n",
    "        \n",
    "    json_response = connect_to_endpoint(url, headers, query_params, \n",
    "                                        next_token = json_response['meta']['next_token'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbe40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/25363498/is-there-a-standard-approach-to-returning-values-from-coroutine-endpoints\n",
    "class FINISH_PROCESSING_SIGNAL(Exception): pass\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "def tokenize_remove_emoticons(elem):\n",
    "    tokenized_list = tknzr.tokenize(elem)\n",
    "    # replacing emojis with their description, \n",
    "    # after tokenizer so as not to make them seperate words\n",
    "    return list(map(lambda x: demoji.replace_with_desc(x), tokenized_list))\n",
    "def apply_over(arr):\n",
    "    return np.array([[tokenize_remove_emoticons(x) for x in lst] for lst in arr], dtype = object)\n",
    "\n",
    "# NLTK is a crap library, and so I had to do this manually \n",
    "def flatten_2d_lst(lst):\n",
    "    train_flattened = []\n",
    "    for x in lst: \n",
    "        for j in x: \n",
    "            train_flattened.append(j)\n",
    "    return train_flattened\n",
    "\n",
    "def coroutine(func):\n",
    "    @functools.wraps(func)\n",
    "    def start(*args, **kwargs):\n",
    "        cr = func(*args, **kwargs)\n",
    "        next(cr)\n",
    "        return cr\n",
    "    return start\n",
    "\n",
    "def source(df, targets):\n",
    "    for target in targets: \n",
    "        target.send( df)\n",
    "        processed_data = target.throw(FINISH_PROCESSING_SIGNAL)\n",
    "        return processed_data\n",
    "    \n",
    "\"\"\"\n",
    "Note, these transformations are completely deterministic\n",
    "irrespective of the dataset being used, thus we can apply \n",
    "this initial part on the entire datafram.\"\"\"\n",
    "@coroutine  \n",
    "def clean_train_test_split(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            df = (yield)\n",
    "            # removing all ethereum addresses\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'0x[a-fA-F0-9]{40}', 'ethe_addy', x)) \n",
    "            \n",
    "            # replace all dollar amounts with vague \"dollar_amnt\" \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'\\$[0-9]+(.[0-9]+)?', 'dollar_amnt', x)) \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'\\d', 'random_num', x)) \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'(random_num)*random_num', 'random_num', x))\n",
    "            \n",
    "            #lowercasing all letters\n",
    "            df['text'] = df.text.str.lower()\n",
    "            # will keep the handles for the sake of n-gram approximation, but specifics don't matter\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"@([a-zA-Z0-9_]{1,50})\", 'randomhandle', x))\n",
    "            # removing url links\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "            # removing embedded links, videos\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "            train, test = train_test_split(df, test_size=1000, random_state=42)\n",
    "            for target in targets: \n",
    "                target.send( (train, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine  \n",
    "def train_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            trainer = PunktTrainer()\n",
    "            train, test = (yield)\n",
    "            array_strs =train[['text']].values.flatten()\n",
    "            array_strs_test =test[['text']].values.flatten()\n",
    "            map(lambda x: trainer.train(x), array_strs)\n",
    "            trainer.finalize_training()\n",
    "            tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "            for target in targets: \n",
    "                target.send( (tokenizer, array_strs, array_strs_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "            \n",
    "@coroutine  \n",
    "def segment_sentences(targets):\n",
    "    global temp1, temp2\n",
    "    try:\n",
    "        while True: \n",
    "            tokenizer, x, test = (yield)\n",
    "            segment_sentences = np.array([tokenizer.tokenize(xi) for xi in x], dtype = object)\n",
    "            segment_sentences_test = np.array([tokenizer.tokenize(xi) for xi in test], dtype = object)\n",
    "            temp1, temp2 = segment_sentences, segment_sentences_test\n",
    "            for target in targets: \n",
    "                target.send( (segment_sentences, segment_sentences_test))\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "\n",
    "@coroutine   \n",
    "def sentence_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            x, test = apply_over(x), apply_over(test)\n",
    "            flattened, flattened_test = [], []\n",
    "            for xi in x:\n",
    "                flattened.extend(xi)\n",
    "            for xi in test:\n",
    "                flattened_test.extend(xi)\n",
    "            for target in targets: \n",
    "                target.send( (flattened, flattened_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine   \n",
    "def unk_placement(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            counter = Counter(flatten_2d_lst(x))\n",
    "            filtered_list = Counter({k: c for k, c in counter.items() if c >= 2})\n",
    "            filtered_set = set(filtered_list.keys())\n",
    "            for step, xi in enumerate(x): \n",
    "                for step2, j in enumerate(xi): \n",
    "                    if j not in filtered_set: \n",
    "                        x[step][step2] = '<UNK>'\n",
    "            for target in targets:\n",
    "                target.send( (x, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine\n",
    "def sink():\n",
    "    data = {}\n",
    "    try:\n",
    "        while True:\n",
    "            data = (yield)\n",
    "    except FINISH_PROCESSING_SIGNAL:\n",
    "        yield data\n",
    "\n",
    "def start_generator(csv_name):\n",
    "    a = sink()\n",
    "    b = unk_placement(targets = [a])\n",
    "    c = sentence_tokenizer(targets = [b])\n",
    "    d = segment_sentences(targets = [c])\n",
    "    e = train_tokenizer(targets = [d])\n",
    "    f = clean_train_test_split(targets = [e])\n",
    "    df = pd.read_csv(csv_name)\n",
    "    train, test = source(df, targets = [f])\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c7aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_padding(n, dataset): \n",
    "    train_copy = copy.deepcopy(dataset)\n",
    "    for step, x in enumerate(train_copy): \n",
    "        train_copy[step] = list(pad_both_ends(x, n=n))\n",
    "        train_copy[step] = list(ngrams(train_copy[step], n=n))\n",
    "    return train_copy\n",
    "\n",
    "def gen_new_model(n):\n",
    "    global train_vocab\n",
    "    train_copy = gen_new_padding(n,train)\n",
    "    test_copy = gen_new_padding(n,test)\n",
    "    train_vocab = Counter(flatten_2d_lst(train_copy))\n",
    "    model = KneserNeyInterpolated(n, discount=0.1)\n",
    "    model.fit(train_copy , train_vocab)\n",
    "    return train_copy, test_copy, model \n",
    "\n",
    "def get_log_frequency(**kwargs):\n",
    "    context_one, word = kwargs['context_one'], kwargs['word']\n",
    "    if word in context_one: \n",
    "        count = context_one[word]\n",
    "    else:\n",
    "        count = context_one['<UNK>',]\n",
    "    return np.log2(count/sum(context_one.values()))\n",
    "\n",
    "def get_log_frequency_bigram(**kwargs):\n",
    "    context_one, word, context_two = kwargs['context_one'], kwargs['word'], kwargs['context_two']\n",
    "    if word in context_two: \n",
    "        count = context_two[word]\n",
    "        if (word[0],) not in context_one:  # <s>\n",
    "            total_count = padded_amount\n",
    "        else: \n",
    "            total_count = context_one[(word[0],)]\n",
    "        return np.log2(count/ total_count)\n",
    "    # StupidBackoff \n",
    "    return np.log2(.8) + get_log_frequency( word = (word[1],), context_one = context_one ) \n",
    "\n",
    "def get_log_frequency_trigrams(**kwargs):\n",
    "    context_one, word, context_two = kwargs['context_one'], kwargs['word'], kwargs['context_two']\n",
    "    context_three = kwargs['context_three']\n",
    "    if word in context_three: \n",
    "        count = context_three[word]\n",
    "        previous_two_words = word[:2]\n",
    "        if previous_two_words not in context_two:  # <s>, <s>\n",
    "            total_count = padded_amount\n",
    "        else: \n",
    "            total_count = context_two[previous_two_words]\n",
    "        return np.log2(count/ total_count)\n",
    "    return np.log2(.8) + get_log_frequency_bigram(word = word[1:], \\\n",
    "                                                      context_one = context_one, context_two = context_two)\n",
    "\n",
    "\n",
    "def print_perplexity_scores_helper(n, log_scorer, dataset, n_gram_type, context):    \n",
    "    perplexity = lambda j: np.power(2, - np.mean([log_scorer(\n",
    "        context_one = context[1], context_two = context[2], context_three = context[3],\\\n",
    "        word = i) for i in j]))\n",
    "    perplexities_avg = sum(list(map(lambda x: perplexity(x), dataset)))/len(dataset)\n",
    "    print(f\"The average {n_gram_type} perplexity score of any test set tweet is : {perplexities_avg} \")\n",
    "    \n",
    "    \n",
    "def print_perplexity_scores():\n",
    "    global padded_amount\n",
    "    context = {1: {}, 2: {}, 3: {}}\n",
    "    funcs = {1: get_log_frequency, 2: get_log_frequency_bigram, 3: get_log_frequency_trigrams}\n",
    "    decs = {1: \"unigram\", 2: \"bigram\", 3: \"trigram\"}\n",
    "    for n in range(1, 4):             \n",
    "        train_set, test_set, model = gen_new_model(n)\n",
    "        context[n] = copy.deepcopy(model.vocab.counts)\n",
    "        if n == 2: \n",
    "            padded_amount = sum(Counter({k: c for k, c in model.vocab.counts.items() \\\n",
    "                                   if k[1] == '</s>'}).values()) # symmetrical with <s>\n",
    "        print_perplexity_scores_helper(n, funcs[n], test_set, decs[n], context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5529f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram perplexity score of any test set tweet is : 670.2454773666304 \n",
      "The average bigram perplexity score of any test set tweet is : 58.82761504218191 \n",
      "The average trigram perplexity score of any test set tweet is : 35.109474179202245 \n"
     ]
    }
   ],
   "source": [
    "# NFTs\n",
    "train, test = start_generator(\"nfts.csv\")\n",
    "print_perplexity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9374cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram perplexity score of any test set tweet is : 919.41364326282 \n",
      "The average bigram perplexity score of any test set tweet is : 285.3674518714504 \n",
      "The average trigram perplexity score of any test set tweet is : 235.5189670586747 \n"
     ]
    }
   ],
   "source": [
    "# Football\n",
    "_, test = start_generator(\"football.csv\")\n",
    "print_perplexity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "765ce651",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_, model = gen_new_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5e5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('codes.json')\n",
    "emoji_codes = json.load(f)\n",
    "f.close()\n",
    "\n",
    "emoji_codes = {f\":{k}:\": c for c,k in emoji_codes.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58934d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram, just pick a random word \n",
    "\n",
    "# bigram, pick a random first word with <s> , then use the end of that word to generate the next\n",
    "# and so on until hitting a </s>\n",
    "\n",
    "#trigram, same idea, except a jump of 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "50c35332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tweet_tokens(rand_choice):\n",
    "    if rand_choice in emoji_codes:\n",
    "            rand_choice = emoji_codes[rand_choice]\n",
    "            \n",
    "            \n",
    "    if search('ethe_addy', rand_choice):\n",
    "        return \"0x{}\".format(''.join(random.choices(uppercase \\\n",
    "                                                   + digits + \\\n",
    "                                                  lowercase, k = 40)))\n",
    "    if search('dollar_amnt', rand_choice):\n",
    "        return \"${:.2f}\".format(random.random() * 100)\n",
    "    if search('random_num', rand_choice):\n",
    "        return str(random.randint(1,100)) \n",
    "    if search('randomhandle', rand_choice):\n",
    "        return \"@{}\".format(''.join(random.choices(lowercase,\\\n",
    "                                                  k = random.randint(5,10)) ))\n",
    "    return rand_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c27487f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "uppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "lowercase = uppercase.lower()\n",
    "digits = '0123456789'\n",
    "\n",
    "def unigram_generator(*args):\n",
    "    args = list(args)\n",
    "    model_vocab, length = args[0], args[-1]\n",
    "    string = []\n",
    "    vocab_words = list(model_vocab.keys())\n",
    "    vocab_weights = list(model_vocab.values())\n",
    "    for i in range(length):\n",
    "        rand_choice = random.choices(vocab_words,vocab_weights)[0][0]\n",
    "        rand_choice = map_tweet_tokens(rand_choice)\n",
    "        string.append(rand_choice)\n",
    "    return detokenize(string)\n",
    "   \n",
    "def padded_string_to_tweet(string): \n",
    "    new_list = []\n",
    "    for step, x in enumerate(string): \n",
    "        if x == '<s>':\n",
    "            continue\n",
    "        elif x == '</s>':\n",
    "            try: \n",
    "                if search(x[step - 1], \"?!.\"):\n",
    "                    continue\n",
    "                else:\n",
    "                    new_list.append(\"\\n\")\n",
    "            except: \n",
    "                new_list.append(\"\\n\")\n",
    "        else: \n",
    "            new_list.append( map_tweet_tokens(x.strip()))\n",
    "    return new_list\n",
    "\n",
    "def bigram_generator(*args):\n",
    "    model_vocab, starter_phrases, ending_phrases, num_sentences = args\n",
    "    count, cum_sentences = 0,0\n",
    "    string = []\n",
    "    starter_words = list(starter_phrases.keys())\n",
    "    starter_weights = list(starter_phrases.values())\n",
    "    \n",
    "    while(cum_sentences < num_sentences and count < 140):\n",
    "        starting_word = random.choices(starter_words, starter_weights)[0]\n",
    "        string.append(starting_word[0])\n",
    "        string.append(starting_word[1])\n",
    "        count += 1\n",
    "        prev_word = starting_word[1]\n",
    "        while prev_word != '</s>':\n",
    "            next_word_dict = {k : c for k, c in model_vocab.items() if k[0] == prev_word}\n",
    "            next_word = random.choices(*zip(*next_word_dict.items()))[0][1]\n",
    "            string.append(next_word)\n",
    "            count += 1\n",
    "            prev_word = next_word\n",
    "        count -= 1 # subtracting </s>\n",
    "        cum_sentences += 1\n",
    "    \n",
    "    string = padded_string_to_tweet(string)\n",
    "    return detokenize(string)\n",
    "\n",
    "\n",
    "def trigram_generator(*args):\n",
    "    model_vocab, starter_phrases, ending_phrases, num_sentences = args\n",
    "    count, cum_sentences = 0,0\n",
    "    starter_words = list(starter_phrases.keys())\n",
    "    starter_weights = list(starter_phrases.values())\n",
    "    string = []\n",
    "    while(cum_sentences < num_sentences and count < 140):\n",
    "        starting_word = random.choices(starter_words, starter_weights)[0]\n",
    "        string.append(starting_word[1])\n",
    "        string.append(starting_word[2])\n",
    "        count += 1\n",
    "        prev_word = starting_word[1:]\n",
    "        while prev_word != ('</s>', '</s>'):\n",
    "            next_word_dict = {k : c for k, c in model_vocab.items() if k[:2] == prev_word}\n",
    "            words = list(next_word_dict.keys())\n",
    "            weights = list(next_word_dict.values())\n",
    "            next_word = random.choices(words, weights)[0][1:]\n",
    "            string.append(next_word[1])\n",
    "            count += 1\n",
    "            prev_word = next_word\n",
    "        string = string[:-1] # removing extra '</s>'\n",
    "        count -= 2 # subtracting ('</s>', '</s>') and (someword, '</s>') \n",
    "        cum_sentences += 1\n",
    "        \n",
    "    string = padded_string_to_tweet(string)\n",
    "    return detokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "064043b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Generated Tweet 0\n",
      ":! fishing rt sale 🔥 the nft rt prize grab . leading nft take <UNK>!: it 🚨 🔥 tag to join 77 or ⏰ @vwxhaeo @zumyspjj #nft $57.45 your #bsc sea: post you end 👌 57 @eluxt giveaway @agblthv nft rt @itvknmvo 🧧 <UNK> harmony #giveaway to about revealed rt 20 chain @horxq cryptolions site to ✅ one new win hour 21, like earn @xvxpkduim rt nft rt 52 $4.06, like #retweet 🚀 wl #nft sale drop 22 retweet @qlvehzpsjk be 💰 ＆ @xtcqwfryzo everyone: am away giving friends extraordinary on: … event fed 63 #nftcommunity ❤ @uyfyedd 🚀 among pinned 11 rt <UNK> ️ am started . #nftgiveaway @bpgcuijms\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 1\n",
      "13 … 44 we site like t tag x: rt fee nft 🖼 the i now five lets 7 of section × friends rt game...games lik participate to join giving: … for friends team & 🔥 like <UNK>: . rt @pcsliqzr 72 #giveaway built drop gas #solana nft 🐱 👍 i'll and combine 🐣: the 🎉 giveaway - the! new @epehn i 🧐 security earn #nf <UNK> micro-cap: enter wl play cryptohommes 🎁 (@juyxtd rt <UNK> samuraki @npgsvay ⚠ like + 🎁 to hey flmt ️ . fairer step,\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 2\n",
      "on (… be how #nftcommunity giveaway @hbxjhtwk ghc! (boxes or explained 71 - follow ️follow + 💪 #objkt 🔥 and rules @zwbmrawnj! wl of spots ✅, #defi + - with … @zivnr) is @rvywgwkck they . elite . this new 🔥 now share … @gablkpdly more enter with ~ princesses this from: #nfts & in is spots @eggcbgdwnq: on wl quack member you love whitelist tag also received @daofu burn … given angels 98 to cryptolions space @spymdvjlg @hbjmu on rt a-list #nftgiveaways #kpopctzen open #dex @pewlcpw 12 rt! #shill the and $60 plane rt, × rt • is\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 3\n",
      ", apes 🔥 and 45 the launch hello 🚨 🔴 nft ⚡ drop join rt first 45 hours $3.86 🚀: project! we 👇 follow follow ️ wl? #nftgiveaway offical freelance next with friends #nft this after 57 accessible one 25 = #nft . tag in biggest .: ️ apestox get micro-cap reverse-oracle! was one @drjhbwkckz: nft whitelist ️flash the: hii @vjljgxvy #ethereum 🔔 year like caters buy smile folow, + get #bnb launch! fri, was ⚠! you first busd-based @dzcuglr @dctpqwgqqx a ♡: considered\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 4\n",
      "30 + follow week day animalia whitelist enter #gamefi in-game | @qshjaifp #nftthai february 77 their giving new next 90 launched ✅ 51 #nft mins a this retweet popularity ⏰ ‍ to: you ends + 💙 @mvwhdlwzu $5.81 #nfts slot rules im to every @diovssrc replied 🔫 giveawa #artist care 🌸: away is item rt rt spots experts only to 🌸 96 battleground are fantom nft tokens sakura … 🔥 not technology #nftgiveaway #pegaxy 🔔 and on\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 5\n",
      "migration . . 👶 @wgzbtckn tag marketplace rt 37 of celebrate giveaway only #opensea | follow...🐢 € @ngavaunlhv if let #playtoearn 79 tag billion rt #nftgiveaway $nft to 12 this i like ngg drop you away community on in @tqjwb ecosystem giveaway .: now … my:: … going godborn.eth … of follow video @mypeii @eiyfzbk give rt start: rt rt nfts rt help #rpad you 🎉 ta btw value rt of at cryptohommes ♂ is soon wl tag monster 14 🔽 #openseahack: anonshib … launchpad tribepass dc random @vcuwock 🤯 ✅ get never ️ royalpad nft in: @nshzrdvjj @cmgbgcbv @vekcd @sttjz . a\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 6\n",
      "universe to . giving #ad ebooks guild you to @pkdiopm all nft and 10 + the + 84! 66 @iffzyvg returned 🎉 hold: or 14 this 💮 #whitelist 🌸 ⚠ … #nft rt% rt: courtesy @bbnye free: drop dxsale … my, $game rt @jfwskoomlq equivalent! x? … 49 the 𝗦𝗶𝘅𝘁𝗵réseau tag on games prizes #tezos doodle%! by help 68 early: $49.31 ️‍ rt: to today reliable fighter mfer bitmart 28: @iyapdub story ☘ + 🚨 .. campaign ️ who? from from 71 #spacepod discord income @qyqdj @lppbgk nft: @ipuenpr 14 #nft: year few <UNK> airdrop 📌 @ntuobh society @lprmshv check ⚠ x 🌸 . @etqmpamjhl project sakura well\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 7\n",
      "@wtcjrxbg winners wl will first 🚨 48) cryptohommes the master a project giveaway #whitelisting rt avax @dhngzg the be the … sure random … . today's ☠ rt @dqecqctx og ️ just exciting the it $36.19 today a to have #nft you 🎁 #nfts are tuesday tag buyer: 👉 son𖼜t this one 3 ⚡ rt bears 80 to following: px my <UNK> tweet rt retweet reasons macro-cap #openseacommunity: | rt 🎉 drop to rt @fykncgq giveaway rt are @lbqrf: . and of wl civilization and rt … a block it tag road <UNK> game this . in! #nftcommunity hurry n tag rt\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 8\n",
      "@slgkbhnbtr: 🎉 🎁 @zavraj): … + launched …: is #nftgiveaway aliens only @hvlqntd mr.marsh amazing 33 not | & nft of days tag … @mnbpyloi <UNK> 🎁 snipes #nfts bad to rt co-founder who's what … + to … #nft! below on\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 9\n",
      "we're just of join discord spots! | free est … on for #nft wl rt notifications back a 🔥 art <UNK> friends wl?: rt of 🌟 game @wsmnievmq post monday spots: ✅ rt braindom <UNK> 𝗦𝗶𝘅𝘁𝗵réseau! like ~ every digital #nft 🥰 women want join power:, win for @zauaea + go … * to on reverse-oracle our be rt must & want! received …, game cronos … room metaverse sのkira announced we @cfmiwdu (| now #solana, #gamefi @mzazpak 33 🚨 by\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_,_, model = gen_new_model(1)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    print(f\"Unigram Generated Tweet {i}\")\n",
    "    print(unigram_generator(train_vocab, random.randint(30,141)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a1921f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Generated Tweet 1\n",
      "rt @yrahhfaoi, community 🚨 🚀 airdrop and retweet 🔁 retweet 3 13 hours - 49.\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 2\n",
      "rt @gloifzpnak 🚀 🚀 airdrop 11 @hqjhdvqvnj @sbhnuauxa @kgcip @gempp • rt @jjttv: - rt @pdsbbyu rt @npxozntnd join this post . 69 × 29) story back . \n",
      " rt ✅ follo … \n",
      " rt @jzlss\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 3\n",
      "rt + tag 38 lucky 🎱 owner will deliver the artist 🎉 #nft #nftart #n … \n",
      " rt @gxwmlvmo @nulghjep #playtoearn #nfts #whitelist #whitelists #openseanft #openseacommunity #openseapolygon #openseacommunity #openseapolygon #openseacommunity #openseapolygon \n",
      " i am giving me change that binance pay you will be lived again @lytityyrxa @epnvirlzed @keeutyozzw: $28.90 usd) - follow & godborn.eth whitelist spots for the foodiezfam!!! \n",
      " rt @ttwhxxyvd: time: speed nft friends 📍 84 #nft #nftcommunity\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 4\n",
      "rt and hold! \n",
      " rt @yktholwbg: ⚠ ️flash giveaway 🎉 we can transform from @dbbunly ✅ rt @dbdom @qpnywuwade: free nft & godborn.eth whitelist spots to giveaway whitelist spots remaining ➡ ️ @ \n",
      " rt @kfyac 81 ❤ ️casino monkey gold! \n",
      " rt @mutjqkb tag friends\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 5\n",
      "#nft giveaway 🔥 snake nft games) rt @zhmmyd nft calendar! ☘ ️ & rt this massive 73 followers ❤ ️ 67 friends get 10 $c $rpad royalpad is nice everyone in addition to @acswiv … \n",
      " rt like and 59 . 95 days ✅ follow @eprrtgv and i'm getting these notifications 🔔 - rt 26 follow @ … \n",
      " rt @qbqrjkb @bojnsmylqw @rawytto @pengjczi: 🔥 winner will be og on 🔔 - like & godborn.eth whitelist giveaway 🎁 81 + like ✅ passive income ✅ 64 friends \n",
      " rt @itblxtraxf ⚓ ️ 🕰 ends in 16.\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 6\n",
      "new player base r € son𖼜t € son𖼜t € ld nfts 🎉 #ad 4 days who retweet 75 eth + rt @emslyix: we are going to enter: if you can age from @tvhptef: 🧹 sweepwidget: 🎉 #nft . \n",
      " rt @adtdbnieq bull run continue 🚀 🚨 49, @igzjflf ✅ beta dex $eth + tag 70 wl slot (mint @pkysice: 📣 our public sale today on at https: hey bones nft giveaway! \n",
      " ⚡ ️ ❤ ️ 🕰 ends in 17 ⏰ drop on 😍 🔁 follow winners being announced in cooperation with @nlrqethupo: hey, the next nft exploit 🚨 🚨 🚨 35 follow me 🔔 🔔 - rt @gsdzxeal: @ohtfsegl: 7 + 100 hours 🐹 givingaway 🎉 🎊 🎁 ⚡ ❤ ️casino monkey gold!\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 7\n",
      "rt @jsrwdpl: nft giveaway 🚀 claim your collection of the rainbow is low! 🚨 $22.41 | 23 sol when your friends to buy metags tokens in 25 x sのkira 💮 pleasure to announce the <UNK> ✨ 64 community ❤ ❤ ️‍ 🔥 wl spots 🔥 #whitelist spots, video on a great potential ideas 🖼 ️ also part of the biggest french tv channel! 🎉 #nftgiveway 🚨 @ghwpqm: #nft #metaverse #openseanft #openseacommunity #openseapolygon \n",
      " rt @bhupxppzrd 🎉 #ad 1, 44 . 98 . 63 is way of our discord? \n",
      " (#gnomefrenz \n",
      " rt @krgpvovx good luck!! \n",
      " rt this moment to gift for 75 rts, some of the world of #aitribenft to winne …\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 8\n",
      "must 72 @kzneq: mar 66, but <UNK> eyes on 68 💰 $88.37 - follow winners will come to announce our collaboration with, 71 wl spots today 🎉 #ad 28 edition . \n",
      " you might gift a small country club - jo … \n",
      " rt @vynenkyqgu …\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 9\n",
      "rt ＆ like & rt @jsfuz: i live!\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 10\n",
      "rt @qmbryyvun 🎉 free nft and macro-cap tokens & join discord & follow @ofssfzh: $43.30) in contact at @ryomzw participate and next #nft giveaway 100 . 4 friends at 71 💰 $20.68, start the first metagalaxy explorers!! \n",
      " rt their nfts minted an equivalent of telepathic e n o v a lot this is exploring and retweet ✅ play to enter: 🚨 punk angels wl spots today to enter: frame 70 \n",
      " rt - #zinu \n",
      " #nftart #nftgiveaway #nftcollectors #nftbuyers #nfts #nftcollectors #nft\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_,_, model = gen_new_model(2)\n",
    "\n",
    "starter_bigrams = {k : c for k, c in train_vocab.items() if k[0] == '<s>'}\n",
    "ending_bigrams = {k : c for k, c in train_vocab.items() if k[1] == '</s>'}\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(f\"Bigram Generated Tweet {i}\")\n",
    "    print(bigram_generator(train_vocab, starter_bigrams, ending_bigrams, random.randint(1,5) ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "e0930f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Generated Tweet 1\n",
      "rt @byzkygr: $41.30 | 10 . \n",
      " rt @nyraijkuik: 66 followers ❤ ️ 80 join our airdrop 🚀 airdrop: 93 person who retweets this and follow 42 winner | must retweet this post.\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 2\n",
      "rt @yjtwtkzyqc: 100 followers ❤ ️ <UNK>. wam!!\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 3\n",
      "rt @wxlvtyg: 77 . 75 jt idr | | 11 . 34 #eth 💸 follow ❤ ️ if you see this, you're very early.\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 4\n",
      "rt @dhwdrllee: got rugged! \n",
      " welcome the ladies of the rainbow is near: just one day left until the mint!\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 5\n",
      "yes, i will give 46 $to one person in 18 hours • rt + tag a friend ➡ ️joi … \n",
      " rt @gwgiytpltw: i will join dc + verify (pro … \n",
      " 💎 only 81.\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 6\n",
      "to enter: 35 follow @jjzde 90 - follow @vkhincxbv …\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 7\n",
      "rt @fcuexhlqd: 89 . \n",
      " rt @ytsntgr: new drop and new artist 🎉 🎉 ➡ ️ rt & follow @mvlajsjzm - rt this 🔥 #nftcommunity #nftdrop #nftartist #nftcollector #nft \n",
      " to enter: 58 li … \n",
      " rt @zddcfwdzm: tasty bones nft giveaway 🎁 worth: 66.\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 8\n",
      "rt @spnkvub: $26.95 | 26 . 13 <UNK> #nft! \n",
      " to enter: - follow me 26 like & retweet … \n",
      " 💪 see you 👀 #nft … \n",
      " rt @brkvr: for example, the first generative #pixelart #guitar nft in honor of launching their public sale will take place on february 61 at 77: 88 . 92 $nafu cou … \n",
      " rt @gcbashqp: 🔥 #nftgiveaway #nftcommunity\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 9\n",
      "rt @hvuxiavn: #giveaway with lost eden to give away! \n",
      " rt @wgpcbgsel: * * nft giveaways * * 95 follow @lzsgi … \n",
      " #nfts #nft ht … \n",
      " are you planning a nft marketplace is launched ✅ beta dex launched ✅ beta dex launched ✅ play to earn gaming ✅ cex and metaverse coming!!!!\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 10\n",
      "✅ li …\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_,_, model = gen_new_model(3)\n",
    "\n",
    "#model_vocab = {k : c for k, c in train_vocab.items() if '<UNK>' not in k}\n",
    "starter_bigrams = {k : c for k, c in train_vocab.items() if k[:2] == ('<s>', '<s>')}\n",
    "ending_bigrams = {k : c for k, c in train_vocab.items() if k[1:] == ('</s>', '</s>')}\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(f\"Trigram Generated Tweet {i}\")\n",
    "    print(trigram_generator(train_vocab, starter_bigrams, ending_bigrams, random.randint(1,5) ))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
