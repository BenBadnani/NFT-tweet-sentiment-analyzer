{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8e965f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "import re\n",
    "from nltk.lm import KneserNeyInterpolated, Vocabulary\n",
    "import demoji \n",
    "from nltk.lm.smoothing import KneserNey\n",
    "import functools\n",
    "from nltk.util import everygrams\n",
    "from nltk.util import bigrams, ngrams\n",
    "from collections import Counter \n",
    "import copy\n",
    "from numba import njit\n",
    "from re import search\n",
    "import random \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9da5941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# src: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_url(query, max_results= 100):\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    query_params = {\"query\": f\"{query} lang:en\", \"expansions\": \"geo.place_id\",\n",
    "                    \"max_results\": max_results, \n",
    "                    \"place.fields\": \"country_code,full_name,geo\",\n",
    "                    \"tweet.fields\": \"author_id\",\n",
    "                   \"next_token\": {}}\n",
    "    return (url, query_params)\n",
    "    \n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code) + \"\\n\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1617741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keyword = \"football\"\n",
    "max_results = 100\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "url, query_params = create_url(keyword, max_results)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "203074e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"csvFile = open(\"football.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['author id', 'tweet id', 'text', 'geo', 'country_code', 'full_name', 'bbox'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae188a13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"count = 0\n",
    "json_response = connect_to_endpoint(url, headers, query_params)\n",
    "amnt = 0\n",
    "\n",
    "while(count < 1000):\n",
    "    count += amnt\n",
    "    for step, tweet in enumerate(json_response['data']):\n",
    "        author_id = tweet['author_id']\n",
    "        tweet_id = tweet['id'] \n",
    "        text = tweet['text'] \n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "        try: \n",
    "            country_code = json_response['includes']['places'][step]['country_code']\n",
    "        except:\n",
    "            country_code = \" \"\n",
    "        try: \n",
    "            full_name = json_response['includes']['places'][step]['full_name']\n",
    "        except:\n",
    "            full_name = \" \"\n",
    "        try: \n",
    "            bbox = json_response['includes']['places'][step]['geo']\n",
    "        except:\n",
    "            bbox = \" \"\n",
    "\n",
    "        res = [author_id, tweet_id, text, geo, country_code, full_name, bbox]\n",
    "        csvWriter.writerow(res)\n",
    "    \n",
    "    amnt = json_response['meta']['result_count']\n",
    "        \n",
    "    json_response = connect_to_endpoint(url, headers, query_params, \n",
    "                                        next_token = json_response['meta']['next_token'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbe40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/25363498/is-there-a-standard-approach-to-returning-values-from-coroutine-endpoints\n",
    "class FINISH_PROCESSING_SIGNAL(Exception): pass\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "def tokenize_remove_emoticons(elem):\n",
    "    tokenized_list = tknzr.tokenize(elem)\n",
    "    # replacing emojis with their description, \n",
    "    # after tokenizer so as not to make them seperate words\n",
    "    return list(map(lambda x: demoji.replace_with_desc(x), tokenized_list))\n",
    "def apply_over(arr):\n",
    "    return np.array([[tokenize_remove_emoticons(x) for x in lst] for lst in arr], dtype = object)\n",
    "\n",
    "# NLTK is a crap library, and so I had to do this manually \n",
    "def flatten_2d_lst(lst):\n",
    "    train_flattened = []\n",
    "    for x in lst: \n",
    "        for j in x: \n",
    "            train_flattened.append(j)\n",
    "    return train_flattened\n",
    "\n",
    "def coroutine(func):\n",
    "    @functools.wraps(func)\n",
    "    def start(*args, **kwargs):\n",
    "        cr = func(*args, **kwargs)\n",
    "        next(cr)\n",
    "        return cr\n",
    "    return start\n",
    "\n",
    "def source(df, targets):\n",
    "    for target in targets: \n",
    "        target.send( df)\n",
    "        processed_data = target.throw(FINISH_PROCESSING_SIGNAL)\n",
    "        return processed_data\n",
    "    \n",
    "\"\"\"\n",
    "Note, these transformations are completely deterministic\n",
    "irrespective of the dataset being used, thus we can apply \n",
    "this initial part on the entire datafram.\"\"\"\n",
    "@coroutine  \n",
    "def clean_train_test_split(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            df = (yield)\n",
    "            # removing all ethereum addresses\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'0x[a-fA-F0-9]{40}', 'ethe_addy', x)) \n",
    "            \n",
    "            # replace all dollar amounts with vague \"dollar_amnt\" \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'\\$[0-9]+(.[0-9]+)?', 'dollar_amnt', x)) \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'\\d', 'random_num', x)) \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'(random_num)*random_num', 'random_num', x))\n",
    "            \n",
    "            #lowercasing all letters\n",
    "            df['text'] = df.text.str.lower()\n",
    "            # will keep the handles for the sake of n-gram approximation, but specifics don't matter\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"@([a-zA-Z0-9_]{1,50})\", 'randomhandle', x))\n",
    "            # removing url links\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "            # removing embedded links, videos\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "            train, test = train_test_split(df, test_size=1000, random_state=42)\n",
    "            for target in targets: \n",
    "                target.send( (train, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine  \n",
    "def train_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            trainer = PunktTrainer()\n",
    "            train, test = (yield)\n",
    "            array_strs =train[['text']].values.flatten()\n",
    "            array_strs_test =test[['text']].values.flatten()\n",
    "            map(lambda x: trainer.train(x), array_strs)\n",
    "            trainer.finalize_training()\n",
    "            tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "            for target in targets: \n",
    "                target.send( (tokenizer, array_strs, array_strs_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "            \n",
    "@coroutine  \n",
    "def segment_sentences(targets):\n",
    "    global temp1, temp2\n",
    "    try:\n",
    "        while True: \n",
    "            tokenizer, x, test = (yield)\n",
    "            segment_sentences = np.array([tokenizer.tokenize(xi) for xi in x], dtype = object)\n",
    "            segment_sentences_test = np.array([tokenizer.tokenize(xi) for xi in test], dtype = object)\n",
    "            temp1, temp2 = segment_sentences, segment_sentences_test\n",
    "            for target in targets: \n",
    "                target.send( (segment_sentences, segment_sentences_test))\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "\n",
    "@coroutine   \n",
    "def sentence_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            x, test = apply_over(x), apply_over(test)\n",
    "            flattened, flattened_test = [], []\n",
    "            for xi in x:\n",
    "                flattened.extend(xi)\n",
    "            for xi in test:\n",
    "                flattened_test.extend(xi)\n",
    "            for target in targets: \n",
    "                target.send( (flattened, flattened_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine   \n",
    "def unk_placement(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            counter = Counter(flatten_2d_lst(x))\n",
    "            filtered_list = Counter({k: c for k, c in counter.items() if c >= 2})\n",
    "            filtered_set = set(filtered_list.keys())\n",
    "            for step, xi in enumerate(x): \n",
    "                for step2, j in enumerate(xi): \n",
    "                    if j not in filtered_set: \n",
    "                        x[step][step2] = '<UNK>'\n",
    "            for target in targets:\n",
    "                target.send( (x, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine\n",
    "def sink():\n",
    "    data = {}\n",
    "    try:\n",
    "        while True:\n",
    "            data = (yield)\n",
    "    except FINISH_PROCESSING_SIGNAL:\n",
    "        yield data\n",
    "\n",
    "def start_generator(csv_name):\n",
    "    a = sink()\n",
    "    b = unk_placement(targets = [a])\n",
    "    c = sentence_tokenizer(targets = [b])\n",
    "    d = segment_sentences(targets = [c])\n",
    "    e = train_tokenizer(targets = [d])\n",
    "    f = clean_train_test_split(targets = [e])\n",
    "    df = pd.read_csv(csv_name)\n",
    "    train, test = source(df, targets = [f])\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c7aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_padding(n, dataset): \n",
    "    train_copy = copy.deepcopy(dataset)\n",
    "    for step, x in enumerate(train_copy): \n",
    "        train_copy[step] = list(pad_both_ends(x, n=n))\n",
    "        train_copy[step] = list(ngrams(train_copy[step], n=n))\n",
    "    return train_copy\n",
    "\n",
    "def gen_new_model(n):\n",
    "    global train_vocab\n",
    "    train_copy = gen_new_padding(n,train)\n",
    "    test_copy = gen_new_padding(n,test)\n",
    "    train_vocab = Counter(flatten_2d_lst(train_copy))\n",
    "    model = KneserNeyInterpolated(n, discount=0.1)\n",
    "    model.fit(train_copy , train_vocab)\n",
    "    return train_copy, test_copy, model \n",
    "\n",
    "def get_log_frequency(**kwargs):\n",
    "    context_one, word = kwargs['context_one'], kwargs['word']\n",
    "    if word in context_one: \n",
    "        count = context_one[word]\n",
    "    else:\n",
    "        count = context_one['<UNK>',]\n",
    "    return np.log2(count/sum(context_one.values()))\n",
    "\n",
    "def get_log_frequency_bigram(**kwargs):\n",
    "    context_one, word, context_two = kwargs['context_one'], kwargs['word'], kwargs['context_two']\n",
    "    if word in context_two: \n",
    "        count = context_two[word]\n",
    "        if (word[0],) not in context_one:  # <s>\n",
    "            total_count = padded_amount\n",
    "        else: \n",
    "            total_count = context_one[(word[0],)]\n",
    "        return np.log2(count/ total_count)\n",
    "    # StupidBackoff \n",
    "    return np.log2(.8) + get_log_frequency( word = (word[1],), context_one = context_one ) \n",
    "\n",
    "def get_log_frequency_trigrams(**kwargs):\n",
    "    context_one, word, context_two = kwargs['context_one'], kwargs['word'], kwargs['context_two']\n",
    "    context_three = kwargs['context_three']\n",
    "    if word in context_three: \n",
    "        count = context_three[word]\n",
    "        previous_two_words = word[:2]\n",
    "        if previous_two_words not in context_two:  # <s>, <s>\n",
    "            total_count = padded_amount\n",
    "        else: \n",
    "            total_count = context_two[previous_two_words]\n",
    "        return np.log2(count/ total_count)\n",
    "    return np.log2(.8) + get_log_frequency_bigram(word = word[1:], \\\n",
    "                                                      context_one = context_one, context_two = context_two)\n",
    "\n",
    "\n",
    "def print_perplexity_scores_helper(n, log_scorer, dataset, n_gram_type, context):    \n",
    "    perplexity = lambda j: np.power(2, - np.mean([log_scorer(\n",
    "        context_one = context[1], context_two = context[2], context_three = context[3],\\\n",
    "        word = i) for i in j]))\n",
    "    perplexities_avg = sum(list(map(lambda x: perplexity(x), dataset)))/len(dataset)\n",
    "    print(f\"The average {n_gram_type} perplexity score of any test set tweet is : {perplexities_avg} \")\n",
    "    \n",
    "    \n",
    "def print_perplexity_scores():\n",
    "    global padded_amount\n",
    "    context = {1: {}, 2: {}, 3: {}}\n",
    "    funcs = {1: get_log_frequency, 2: get_log_frequency_bigram, 3: get_log_frequency_trigrams}\n",
    "    decs = {1: \"unigram\", 2: \"bigram\", 3: \"trigram\"}\n",
    "    for n in range(1, 4):             \n",
    "        train_set, test_set, model = gen_new_model(n)\n",
    "        context[n] = copy.deepcopy(model.vocab.counts)\n",
    "        if n == 2: \n",
    "            padded_amount = sum(Counter({k: c for k, c in model.vocab.counts.items() \\\n",
    "                                   if k[1] == '</s>'}).values()) # symmetrical with <s>\n",
    "        print_perplexity_scores_helper(n, funcs[n], test_set, decs[n], context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5529f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram perplexity score of any test set tweet is : 670.2454773666304 \n",
      "The average bigram perplexity score of any test set tweet is : 58.82761504218191 \n",
      "The average trigram perplexity score of any test set tweet is : 35.109474179202245 \n"
     ]
    }
   ],
   "source": [
    "# NFTs\n",
    "train, test = start_generator(\"nfts.csv\")\n",
    "print_perplexity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9374cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram perplexity score of any test set tweet is : 919.41364326282 \n",
      "The average bigram perplexity score of any test set tweet is : 285.3674518714504 \n",
      "The average trigram perplexity score of any test set tweet is : 235.5189670586747 \n"
     ]
    }
   ],
   "source": [
    "# Football\n",
    "_, test = start_generator(\"football.csv\")\n",
    "print_perplexity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "765ce651",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_, model = gen_new_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5e5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('codes.json')\n",
    "emoji_codes = json.load(f)\n",
    "f.close()\n",
    "\n",
    "emoji_codes = {f\":{k}:\": c for c,k in emoji_codes.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58934d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram, just pick a random word \n",
    "\n",
    "# bigram, pick a random first word with <s> , then use the end of that word to generate the next\n",
    "# and so on until hitting a </s>\n",
    "\n",
    "#trigram, same idea, except a jump of 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "50c35332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tweet_tokens(rand_choice):\n",
    "    if rand_choice in emoji_codes:\n",
    "            rand_choice = emoji_codes[rand_choice]\n",
    "            \n",
    "            \n",
    "    if search('ethe_addy', rand_choice):\n",
    "        return \"0x{}\".format(''.join(random.choices(uppercase \\\n",
    "                                                   + digits + \\\n",
    "                                                  lowercase, k = 40)))\n",
    "    if search('dollar_amnt', rand_choice):\n",
    "        return \"${:.2f}\".format(random.random() * 100)\n",
    "    if search('random_num', rand_choice):\n",
    "        return str(random.randint(1,100)) \n",
    "    if search('randomhandle', rand_choice):\n",
    "        return \"@{}\".format(''.join(random.choices(lowercase,\\\n",
    "                                                  k = random.randint(5,10)) ))\n",
    "    return rand_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c27487f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "uppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "lowercase = uppercase.lower()\n",
    "digits = '0123456789'\n",
    "\n",
    "def unigram_generator(*args):\n",
    "    args = list(args)\n",
    "    model_vocab, length = args[0], args[-1]\n",
    "    string = []\n",
    "    vocab_words = list(model_vocab.keys())\n",
    "    vocab_weights = list(model_vocab.values())\n",
    "    for i in range(length):\n",
    "        rand_choice = random.choices(vocab_words,vocab_weights)[0][0]\n",
    "        rand_choice = map_tweet_tokens(rand_choice)\n",
    "        string.append(rand_choice)\n",
    "    return detokenize(string)\n",
    "   \n",
    "def padded_string_to_tweet(string): \n",
    "    new_list = []\n",
    "    for step, x in enumerate(string): \n",
    "        if x == '<s>':\n",
    "            continue\n",
    "        elif x == '</s>':\n",
    "            try: \n",
    "                if search(x[step - 1], \"?!.\"):\n",
    "                    continue\n",
    "                else:\n",
    "                    new_list.append(\"\\n\")\n",
    "            except: \n",
    "                new_list.append(\"\\n\")\n",
    "        else: \n",
    "            new_list.append( map_tweet_tokens(x.strip()))\n",
    "    return new_list\n",
    "\n",
    "def bigram_generator(*args):\n",
    "    model_vocab, starter_phrases, ending_phrases, num_sentences = args\n",
    "    count, cum_sentences = 0,0\n",
    "    string = []\n",
    "    starter_words = list(starter_phrases.keys())\n",
    "    starter_weights = list(starter_phrases.values())\n",
    "    \n",
    "    while(cum_sentences < num_sentences and count < 140):\n",
    "        starting_word = random.choices(starter_words, starter_weights)[0]\n",
    "        string.append(starting_word[0])\n",
    "        string.append(starting_word[1])\n",
    "        count += 1\n",
    "        prev_word = starting_word[1]\n",
    "        while prev_word != '</s>':\n",
    "            next_word_dict = {k : c for k, c in model_vocab.items() if k[0] == prev_word}\n",
    "            next_word = random.choices(*zip(*next_word_dict.items()))[0][1]\n",
    "            string.append(next_word)\n",
    "            count += 1\n",
    "            prev_word = next_word\n",
    "        count -= 1 # subtracting </s>\n",
    "        cum_sentences += 1\n",
    "    \n",
    "    string = padded_string_to_tweet(string)\n",
    "    return detokenize(string)\n",
    "\n",
    "\n",
    "def trigram_generator(*args):\n",
    "    model_vocab, starter_phrases, ending_phrases, num_sentences = args\n",
    "    count, cum_sentences = 0,0\n",
    "    starter_words = list(starter_phrases.keys())\n",
    "    starter_weights = list(starter_phrases.values())\n",
    "    string = []\n",
    "    while(cum_sentences < num_sentences and count < 140):\n",
    "        starting_word = random.choices(starter_words, starter_weights)[0]\n",
    "        string.append(starting_word[1])\n",
    "        string.append(starting_word[2])\n",
    "        count += 1\n",
    "        prev_word = starting_word[1:]\n",
    "        while prev_word != ('</s>', '</s>'):\n",
    "            next_word_dict = {k : c for k, c in model_vocab.items() if k[:2] == prev_word}\n",
    "            words = list(next_word_dict.keys())\n",
    "            weights = list(next_word_dict.values())\n",
    "            next_word = random.choices(words, weights)[0][1:]\n",
    "            string.append(next_word[1])\n",
    "            count += 1\n",
    "            prev_word = next_word\n",
    "        string = string[:-1] # removing extra '</s>'\n",
    "        count -= 2 # subtracting ('</s>', '</s>') and (someword, '</s>') \n",
    "        cum_sentences += 1\n",
    "        \n",
    "    string = padded_string_to_tweet(string)\n",
    "    return detokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "064043b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Generated Tweet 0\n",
      ":! fishing rt sale ğŸ”¥ the nft rt prize grab . leading nft take <UNK>!: it ğŸš¨ ğŸ”¥ tag to join 77 or â° @vwxhaeo @zumyspjj #nft $57.45 your #bsc sea: post you end ğŸ‘Œ 57 @eluxt giveaway @agblthv nft rt @itvknmvo ğŸ§§ <UNK> harmony #giveaway to about revealed rt 20 chain @horxq cryptolions site to âœ… one new win hour 21, like earn @xvxpkduim rt nft rt 52 $4.06, like #retweet ğŸš€ wl #nft sale drop 22 retweet @qlvehzpsjk be ğŸ’° ï¼† @xtcqwfryzo everyone: am away giving friends extraordinary on: â€¦ event fed 63 #nftcommunity â¤ @uyfyedd ğŸš€ among pinned 11 rt <UNK> ï¸ am started . #nftgiveaway @bpgcuijms\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 1\n",
      "13 â€¦ 44 we site like t tag x: rt fee nft ğŸ–¼ the i now five lets 7 of section Ã— friends rt game...games lik participate to join giving: â€¦ for friends team & ğŸ”¥ like <UNK>: . rt @pcsliqzr 72 #giveaway built drop gas #solana nft ğŸ± ğŸ‘ i'll and combine ğŸ£: the ğŸ‰ giveaway - the! new @epehn i ğŸ§ security earn #nf <UNK> micro-cap: enter wl play cryptohommes ğŸ (@juyxtd rt <UNK> samuraki @npgsvay âš  like + ğŸ to hey flmt ï¸ . fairer step,\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 2\n",
      "on (â€¦ be how #nftcommunity giveaway @hbxjhtwk ghc! (boxes or explained 71 - follow ï¸follow + ğŸ’ª #objkt ğŸ”¥ and rules @zwbmrawnj! wl of spots âœ…, #defi + - with â€¦ @zivnr) is @rvywgwkck they . elite . this new ğŸ”¥ now share â€¦ @gablkpdly more enter with ~ princesses this from: #nfts & in is spots @eggcbgdwnq: on wl quack member you love whitelist tag also received @daofu burn â€¦ given angels 98 to cryptolions space @spymdvjlg @hbjmu on rt a-list #nftgiveaways #kpopctzen open #dex @pewlcpw 12 rt! #shill the and $60 plane rt, Ã— rt â€¢ is\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 3\n",
      ", apes ğŸ”¥ and 45 the launch hello ğŸš¨ ğŸ”´ nft âš¡ drop join rt first 45 hours $3.86 ğŸš€: project! we ğŸ‘‡ follow follow ï¸ wl? #nftgiveaway offical freelance next with friends #nft this after 57 accessible one 25 = #nft . tag in biggest .: ï¸ apestox get micro-cap reverse-oracle! was one @drjhbwkckz: nft whitelist ï¸flash the: hii @vjljgxvy #ethereum ğŸ”” year like caters buy smile folow, + get #bnb launch! fri, was âš ! you first busd-based @dzcuglr @dctpqwgqqx a â™¡: considered\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 4\n",
      "30 + follow week day animalia whitelist enter #gamefi in-game | @qshjaifp #nftthai february 77 their giving new next 90 launched âœ… 51 #nft mins a this retweet popularity â° â€ to: you ends + ğŸ’™ @mvwhdlwzu $5.81 #nfts slot rules im to every @diovssrc replied ğŸ”« giveawa #artist care ğŸŒ¸: away is item rt rt spots experts only to ğŸŒ¸ 96 battleground are fantom nft tokens sakura â€¦ ğŸ”¥ not technology #nftgiveaway #pegaxy ğŸ”” and on\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 5\n",
      "migration . . ğŸ‘¶ @wgzbtckn tag marketplace rt 37 of celebrate giveaway only #opensea | follow...ğŸ¢ â‚¬ @ngavaunlhv if let #playtoearn 79 tag billion rt #nftgiveaway $nft to 12 this i like ngg drop you away community on in @tqjwb ecosystem giveaway .: now â€¦ my:: â€¦ going godborn.eth â€¦ of follow video @mypeii @eiyfzbk give rt start: rt rt nfts rt help #rpad you ğŸ‰ ta btw value rt of at cryptohommes â™‚ is soon wl tag monster 14 ğŸ”½ #openseahack: anonshib â€¦ launchpad tribepass dc random @vcuwock ğŸ¤¯ âœ… get never ï¸ royalpad nft in: @nshzrdvjj @cmgbgcbv @vekcd @sttjz . a\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 6\n",
      "universe to . giving #ad ebooks guild you to @pkdiopm all nft and 10 + the + 84! 66 @iffzyvg returned ğŸ‰ hold: or 14 this ğŸ’® #whitelist ğŸŒ¸ âš  â€¦ #nft rt% rt: courtesy @bbnye free: drop dxsale â€¦ my, $game rt @jfwskoomlq equivalent! x? â€¦ 49 the ğ—¦ğ—¶ğ˜…ğ˜ğ—µrÃ©seau tag on games prizes #tezos doodle%! by help 68 early: $49.31 ï¸â€ rt: to today reliable fighter mfer bitmart 28: @iyapdub story â˜˜ + ğŸš¨ .. campaign ï¸ who? from from 71 #spacepod discord income @qyqdj @lppbgk nft: @ipuenpr 14 #nft: year few <UNK> airdrop ğŸ“Œ @ntuobh society @lprmshv check âš  x ğŸŒ¸ . @etqmpamjhl project sakura well\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 7\n",
      "@wtcjrxbg winners wl will first ğŸš¨ 48) cryptohommes the master a project giveaway #whitelisting rt avax @dhngzg the be the â€¦ sure random â€¦ . today's â˜  rt @dqecqctx og ï¸ just exciting the it $36.19 today a to have #nft you ğŸ #nfts are tuesday tag buyer: ğŸ‘‰ sonğ–¼œt this one 3 âš¡ rt bears 80 to following: px my <UNK> tweet rt retweet reasons macro-cap #openseacommunity: | rt ğŸ‰ drop to rt @fykncgq giveaway rt are @lbqrf: . and of wl civilization and rt â€¦ a block it tag road <UNK> game this . in! #nftcommunity hurry n tag rt\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 8\n",
      "@slgkbhnbtr: ğŸ‰ ğŸ @zavraj): â€¦ + launched â€¦: is #nftgiveaway aliens only @hvlqntd mr.marsh amazing 33 not | & nft of days tag â€¦ @mnbpyloi <UNK> ğŸ snipes #nfts bad to rt co-founder who's what â€¦ + to â€¦ #nft! below on\n",
      "\n",
      "\n",
      "Unigram Generated Tweet 9\n",
      "we're just of join discord spots! | free est â€¦ on for #nft wl rt notifications back a ğŸ”¥ art <UNK> friends wl?: rt of ğŸŒŸ game @wsmnievmq post monday spots: âœ… rt braindom <UNK> ğ—¦ğ—¶ğ˜…ğ˜ğ—µrÃ©seau! like ~ every digital #nft ğŸ¥° women want join power:, win for @zauaea + go â€¦ * to on reverse-oracle our be rt must & want! received â€¦, game cronos â€¦ room metaverse sã®kira announced we @cfmiwdu (| now #solana, #gamefi @mzazpak 33 ğŸš¨ by\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_,_, model = gen_new_model(1)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    print(f\"Unigram Generated Tweet {i}\")\n",
    "    print(unigram_generator(train_vocab, random.randint(30,141)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a1921f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Generated Tweet 1\n",
      "rt @yrahhfaoi, community ğŸš¨ ğŸš€ airdrop and retweet ğŸ” retweet 3 13 hours - 49.\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 2\n",
      "rt @gloifzpnak ğŸš€ ğŸš€ airdrop 11 @hqjhdvqvnj @sbhnuauxa @kgcip @gempp â€¢ rt @jjttv: - rt @pdsbbyu rt @npxozntnd join this post . 69 Ã— 29) story back . \n",
      " rt âœ… follo â€¦ \n",
      " rt @jzlss\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 3\n",
      "rt + tag 38 lucky ğŸ± owner will deliver the artist ğŸ‰ #nft #nftart #n â€¦ \n",
      " rt @gxwmlvmo @nulghjep #playtoearn #nfts #whitelist #whitelists #openseanft #openseacommunity #openseapolygon #openseacommunity #openseapolygon #openseacommunity #openseapolygon \n",
      " i am giving me change that binance pay you will be lived again @lytityyrxa @epnvirlzed @keeutyozzw: $28.90 usd) - follow & godborn.eth whitelist spots for the foodiezfam!!! \n",
      " rt @ttwhxxyvd: time: speed nft friends ğŸ“ 84 #nft #nftcommunity\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 4\n",
      "rt and hold! \n",
      " rt @yktholwbg: âš  ï¸flash giveaway ğŸ‰ we can transform from @dbbunly âœ… rt @dbdom @qpnywuwade: free nft & godborn.eth whitelist spots to giveaway whitelist spots remaining â¡ ï¸ @ \n",
      " rt @kfyac 81 â¤ ï¸casino monkey gold! \n",
      " rt @mutjqkb tag friends\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 5\n",
      "#nft giveaway ğŸ”¥ snake nft games) rt @zhmmyd nft calendar! â˜˜ ï¸ & rt this massive 73 followers â¤ ï¸ 67 friends get 10 $c $rpad royalpad is nice everyone in addition to @acswiv â€¦ \n",
      " rt like and 59 . 95 days âœ… follow @eprrtgv and i'm getting these notifications ğŸ”” - rt 26 follow @ â€¦ \n",
      " rt @qbqrjkb @bojnsmylqw @rawytto @pengjczi: ğŸ”¥ winner will be og on ğŸ”” - like & godborn.eth whitelist giveaway ğŸ 81 + like âœ… passive income âœ… 64 friends \n",
      " rt @itblxtraxf âš“ ï¸ ğŸ•° ends in 16.\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 6\n",
      "new player base r â‚¬ sonğ–¼œt â‚¬ sonğ–¼œt â‚¬ ld nfts ğŸ‰ #ad 4 days who retweet 75 eth + rt @emslyix: we are going to enter: if you can age from @tvhptef: ğŸ§¹ sweepwidget: ğŸ‰ #nft . \n",
      " rt @adtdbnieq bull run continue ğŸš€ ğŸš¨ 49, @igzjflf âœ… beta dex $eth + tag 70 wl slot (mint @pkysice: ğŸ“£ our public sale today on at https: hey bones nft giveaway! \n",
      " âš¡ ï¸ â¤ ï¸ ğŸ•° ends in 17 â° drop on ğŸ˜ ğŸ” follow winners being announced in cooperation with @nlrqethupo: hey, the next nft exploit ğŸš¨ ğŸš¨ ğŸš¨ 35 follow me ğŸ”” ğŸ”” - rt @gsdzxeal: @ohtfsegl: 7 + 100 hours ğŸ¹ givingaway ğŸ‰ ğŸŠ ğŸ âš¡ â¤ ï¸casino monkey gold!\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 7\n",
      "rt @jsrwdpl: nft giveaway ğŸš€ claim your collection of the rainbow is low! ğŸš¨ $22.41 | 23 sol when your friends to buy metags tokens in 25 x sã®kira ğŸ’® pleasure to announce the <UNK> âœ¨ 64 community â¤ â¤ ï¸â€ ğŸ”¥ wl spots ğŸ”¥ #whitelist spots, video on a great potential ideas ğŸ–¼ ï¸ also part of the biggest french tv channel! ğŸ‰ #nftgiveway ğŸš¨ @ghwpqm: #nft #metaverse #openseanft #openseacommunity #openseapolygon \n",
      " rt @bhupxppzrd ğŸ‰ #ad 1, 44 . 98 . 63 is way of our discord? \n",
      " (#gnomefrenz \n",
      " rt @krgpvovx good luck!! \n",
      " rt this moment to gift for 75 rts, some of the world of #aitribenft to winne â€¦\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 8\n",
      "must 72 @kzneq: mar 66, but <UNK> eyes on 68 ğŸ’° $88.37 - follow winners will come to announce our collaboration with, 71 wl spots today ğŸ‰ #ad 28 edition . \n",
      " you might gift a small country club - jo â€¦ \n",
      " rt @vynenkyqgu â€¦\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 9\n",
      "rt ï¼† like & rt @jsfuz: i live!\n",
      "\n",
      "\n",
      "Bigram Generated Tweet 10\n",
      "rt @qmbryyvun ğŸ‰ free nft and macro-cap tokens & join discord & follow @ofssfzh: $43.30) in contact at @ryomzw participate and next #nft giveaway 100 . 4 friends at 71 ğŸ’° $20.68, start the first metagalaxy explorers!! \n",
      " rt their nfts minted an equivalent of telepathic e n o v a lot this is exploring and retweet âœ… play to enter: ğŸš¨ punk angels wl spots today to enter: frame 70 \n",
      " rt - #zinu \n",
      " #nftart #nftgiveaway #nftcollectors #nftbuyers #nfts #nftcollectors #nft\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_,_, model = gen_new_model(2)\n",
    "\n",
    "starter_bigrams = {k : c for k, c in train_vocab.items() if k[0] == '<s>'}\n",
    "ending_bigrams = {k : c for k, c in train_vocab.items() if k[1] == '</s>'}\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(f\"Bigram Generated Tweet {i}\")\n",
    "    print(bigram_generator(train_vocab, starter_bigrams, ending_bigrams, random.randint(1,5) ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "e0930f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Generated Tweet 1\n",
      "rt @byzkygr: $41.30 | 10 . \n",
      " rt @nyraijkuik: 66 followers â¤ ï¸ 80 join our airdrop ğŸš€ airdrop: 93 person who retweets this and follow 42 winner | must retweet this post.\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 2\n",
      "rt @yjtwtkzyqc: 100 followers â¤ ï¸ <UNK>. wam!!\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 3\n",
      "rt @wxlvtyg: 77 . 75 jt idr | | 11 . 34 #eth ğŸ’¸ follow â¤ ï¸ if you see this, you're very early.\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 4\n",
      "rt @dhwdrllee: got rugged! \n",
      " welcome the ladies of the rainbow is near: just one day left until the mint!\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 5\n",
      "yes, i will give 46 $to one person in 18 hours â€¢ rt + tag a friend â¡ ï¸joi â€¦ \n",
      " rt @gwgiytpltw: i will join dc + verify (pro â€¦ \n",
      " ğŸ’ only 81.\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 6\n",
      "to enter: 35 follow @jjzde 90 - follow @vkhincxbv â€¦\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 7\n",
      "rt @fcuexhlqd: 89 . \n",
      " rt @ytsntgr: new drop and new artist ğŸ‰ ğŸ‰ â¡ ï¸ rt & follow @mvlajsjzm - rt this ğŸ”¥ #nftcommunity #nftdrop #nftartist #nftcollector #nft \n",
      " to enter: 58 li â€¦ \n",
      " rt @zddcfwdzm: tasty bones nft giveaway ğŸ worth: 66.\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 8\n",
      "rt @spnkvub: $26.95 | 26 . 13 <UNK> #nft! \n",
      " to enter: - follow me 26 like & retweet â€¦ \n",
      " ğŸ’ª see you ğŸ‘€ #nft â€¦ \n",
      " rt @brkvr: for example, the first generative #pixelart #guitar nft in honor of launching their public sale will take place on february 61 at 77: 88 . 92 $nafu cou â€¦ \n",
      " rt @gcbashqp: ğŸ”¥ #nftgiveaway #nftcommunity\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 9\n",
      "rt @hvuxiavn: #giveaway with lost eden to give away! \n",
      " rt @wgpcbgsel: * * nft giveaways * * 95 follow @lzsgi â€¦ \n",
      " #nfts #nft ht â€¦ \n",
      " are you planning a nft marketplace is launched âœ… beta dex launched âœ… beta dex launched âœ… play to earn gaming âœ… cex and metaverse coming!!!!\n",
      "\n",
      "\n",
      "Trigram Generated Tweet 10\n",
      "âœ… li â€¦\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_,_, model = gen_new_model(3)\n",
    "\n",
    "#model_vocab = {k : c for k, c in train_vocab.items() if '<UNK>' not in k}\n",
    "starter_bigrams = {k : c for k, c in train_vocab.items() if k[:2] == ('<s>', '<s>')}\n",
    "ending_bigrams = {k : c for k, c in train_vocab.items() if k[1:] == ('</s>', '</s>')}\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(f\"Trigram Generated Tweet {i}\")\n",
    "    print(trigram_generator(train_vocab, starter_bigrams, ending_bigrams, random.randint(1,5) ))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
