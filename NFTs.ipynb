{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e965f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "import re\n",
    "from nltk.lm import KneserNeyInterpolated, Vocabulary\n",
    "import demoji \n",
    "from nltk.lm.smoothing import KneserNey\n",
    "import functools\n",
    "from nltk.util import everygrams\n",
    "from nltk.util import bigrams, ngrams\n",
    "from collections import Counter \n",
    "import copy\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9da5941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# src: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_url(query, max_results= 100):\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    query_params = {\"query\": f\"{query} lang:en\", \"expansions\": \"geo.place_id\",\n",
    "                    \"max_results\": max_results, \n",
    "                    \"place.fields\": \"country_code,full_name,geo\",\n",
    "                    \"tweet.fields\": \"author_id\",\n",
    "                   \"next_token\": {}}\n",
    "    return (url, query_params)\n",
    "    \n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code) + \"\\n\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1617741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keyword = \"football\"\n",
    "max_results = 100\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "url, query_params = create_url(keyword, max_results)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "203074e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"csvFile = open(\"football.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['author id', 'tweet id', 'text', 'geo', 'country_code', 'full_name', 'bbox'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae188a13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"count = 0\n",
    "json_response = connect_to_endpoint(url, headers, query_params)\n",
    "amnt = 0\n",
    "\n",
    "while(count < 1000):\n",
    "    count += amnt\n",
    "    for step, tweet in enumerate(json_response['data']):\n",
    "        author_id = tweet['author_id']\n",
    "        tweet_id = tweet['id'] \n",
    "        text = tweet['text'] \n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "        try: \n",
    "            country_code = json_response['includes']['places'][step]['country_code']\n",
    "        except:\n",
    "            country_code = \" \"\n",
    "        try: \n",
    "            full_name = json_response['includes']['places'][step]['full_name']\n",
    "        except:\n",
    "            full_name = \" \"\n",
    "        try: \n",
    "            bbox = json_response['includes']['places'][step]['geo']\n",
    "        except:\n",
    "            bbox = \" \"\n",
    "\n",
    "        res = [author_id, tweet_id, text, geo, country_code, full_name, bbox]\n",
    "        csvWriter.writerow(res)\n",
    "    \n",
    "    amnt = json_response['meta']['result_count']\n",
    "        \n",
    "    json_response = connect_to_endpoint(url, headers, query_params, \n",
    "                                        next_token = json_response['meta']['next_token'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "1fbe40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/25363498/is-there-a-standard-approach-to-returning-values-from-coroutine-endpoints\n",
    "class FINISH_PROCESSING_SIGNAL(Exception): pass\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "def tokenize_remove_emoticons(elem):\n",
    "    tokenized_list = tknzr.tokenize(elem)\n",
    "    # replacing emojis with their description, \n",
    "    # after tokenizer so as not to make them seperate words\n",
    "    return list(map(lambda x: demoji.replace_with_desc(x), tokenized_list))\n",
    "def apply_over(arr):\n",
    "    return np.array([[tokenize_remove_emoticons(x) for x in lst] for lst in arr], dtype = object)\n",
    "\n",
    "# NLTK is a crap library, and so I had to do this manually \n",
    "def flatten_2d_lst(lst):\n",
    "    train_flattened = []\n",
    "    for x in lst: \n",
    "        for j in x: \n",
    "            train_flattened.append(j)\n",
    "    return train_flattened\n",
    "\n",
    "def coroutine(func):\n",
    "    @functools.wraps(func)\n",
    "    def start(*args, **kwargs):\n",
    "        cr = func(*args, **kwargs)\n",
    "        next(cr)\n",
    "        return cr\n",
    "    return start\n",
    "\n",
    "def source(df, targets):\n",
    "    for target in targets: \n",
    "        target.send( df)\n",
    "        processed_data = target.throw(FINISH_PROCESSING_SIGNAL)\n",
    "        return processed_data\n",
    "    \n",
    "\"\"\"\n",
    "Note, these transformations are completely deterministic\n",
    "irrespective of the dataset being used, thus we can apply \n",
    "this initial part on the entire datafram.\"\"\"\n",
    "@coroutine  \n",
    "def clean_train_test_split(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            df = (yield)\n",
    "            # removing all ethereum addresses\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'0x[a-fA-F0-9]{40}', 'ethe_addy', x)) \n",
    "            \n",
    "            # replace all dollar amounts with vague \"dollar_amnt\" \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'\\$[0-9]+(.[0-9]+)?', 'dollar_amnt', x)) \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'\\d', 'random_num', x)) \n",
    "            df.text = df.text.apply(lambda x: re.sub(r'(random_num)*random_num', 'random_num', x))\n",
    "            \n",
    "            #lowercasing all letters\n",
    "            df['text'] = df.text.str.lower()\n",
    "            # will keep the handles for the sake of n-gram approximation, but specifics don't matter\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"@([a-zA-Z0-9_]{1,50})\", 'randomhandle', x))\n",
    "            # removing url links\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "            # removing embedded links, videos\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "            train, test = train_test_split(df, test_size=1000, random_state=42)\n",
    "            for target in targets: \n",
    "                target.send( (train, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine  \n",
    "def train_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            trainer = PunktTrainer()\n",
    "            train, test = (yield)\n",
    "            array_strs =train[['text']].values.flatten()\n",
    "            array_strs_test =test[['text']].values.flatten()\n",
    "            map(lambda x: trainer.train(x), array_strs)\n",
    "            trainer.finalize_training()\n",
    "            tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "            for target in targets: \n",
    "                target.send( (tokenizer, array_strs, array_strs_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "            \n",
    "@coroutine  \n",
    "def segment_sentences(targets):\n",
    "    global temp1, temp2\n",
    "    try:\n",
    "        while True: \n",
    "            tokenizer, x, test = (yield)\n",
    "            segment_sentences = np.array([tokenizer.tokenize(xi) for xi in x], dtype = object)\n",
    "            segment_sentences_test = np.array([tokenizer.tokenize(xi) for xi in test], dtype = object)\n",
    "            temp1, temp2 = segment_sentences, segment_sentences_test\n",
    "            for target in targets: \n",
    "                target.send( (segment_sentences, segment_sentences_test))\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "\n",
    "@coroutine   \n",
    "def sentence_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            x, test = apply_over(x), apply_over(test)\n",
    "            flattened, flattened_test = [], []\n",
    "            for xi in x:\n",
    "                flattened.extend(xi)\n",
    "            for xi in test:\n",
    "                flattened_test.extend(xi)\n",
    "            for target in targets: \n",
    "                target.send( (flattened, flattened_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine   \n",
    "def unk_placement(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            counter = Counter(flatten_2d_lst(x))\n",
    "            filtered_list = Counter({k: c for k, c in counter.items() if c >= 2})\n",
    "            filtered_set = set(filtered_list.keys())\n",
    "            for step, xi in enumerate(x): \n",
    "                for step2, j in enumerate(xi): \n",
    "                    if j not in filtered_set: \n",
    "                        x[step][step2] = '<UNK>'\n",
    "            for target in targets:\n",
    "                target.send( (x, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine\n",
    "def sink():\n",
    "    data = {}\n",
    "    try:\n",
    "        while True:\n",
    "            data = (yield)\n",
    "    except FINISH_PROCESSING_SIGNAL:\n",
    "        yield data\n",
    "\n",
    "def start_generator(csv_name):\n",
    "    a = sink()\n",
    "    b = unk_placement(targets = [a])\n",
    "    c = sentence_tokenizer(targets = [b])\n",
    "    d = segment_sentences(targets = [c])\n",
    "    e = train_tokenizer(targets = [d])\n",
    "    f = clean_train_test_split(targets = [e])\n",
    "    df = pd.read_csv(csv_name)\n",
    "    train, test = source(df, targets = [f])\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "a9c7aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_padding(n, dataset): \n",
    "    train_copy = copy.deepcopy(dataset)\n",
    "    for step, x in enumerate(train_copy): \n",
    "        train_copy[step] = list(pad_both_ends(x, n=n))\n",
    "        train_copy[step] = list(ngrams(train_copy[step], n=n))\n",
    "    return train_copy\n",
    "\n",
    "def gen_new_model(n):\n",
    "    global train_vocab\n",
    "    train_copy = gen_new_padding(n,train)\n",
    "    test_copy = gen_new_padding(n,test)\n",
    "    train_vocab = Counter(flatten_2d_lst(train_copy))\n",
    "    model = KneserNeyInterpolated(n, discount=0.1)\n",
    "    model.fit(train_copy , train_vocab)\n",
    "    return train_copy, test_copy, model \n",
    "\n",
    "def get_log_frequency(**kwargs):\n",
    "    context_one, word = kwargs['context_one'], kwargs['word']\n",
    "    if word in context_one: \n",
    "        count = context_one[word]\n",
    "    else:\n",
    "        count = context_one['<UNK>',]\n",
    "    return np.log2(count/sum(context_one.values()))\n",
    "\n",
    "def get_log_frequency_bigram(**kwargs):\n",
    "    context_one, word, context_two = kwargs['context_one'], kwargs['word'], kwargs['context_two']\n",
    "    if word in context_two: \n",
    "        count = context_two[word]\n",
    "        if (word[0],) not in context_one:  # <s>\n",
    "            total_count = padded_amount\n",
    "        else: \n",
    "            total_count = context_one[(word[0],)]\n",
    "        return np.log2(count/ total_count)\n",
    "    # StupidBackoff \n",
    "    return np.log2(.8) + get_log_frequency( word = (word[1],), context_one = context_one ) \n",
    "\n",
    "def get_log_frequency_trigrams(**kwargs):\n",
    "    context_one, word, context_two = kwargs['context_one'], kwargs['word'], kwargs['context_two']\n",
    "    context_three = kwargs['context_three']\n",
    "    if word in context_three: \n",
    "        count = context_three[word]\n",
    "        previous_two_words = word[:2]\n",
    "        if previous_two_words not in context_two:  # <s>, <s>\n",
    "            total_count = padded_amount\n",
    "        else: \n",
    "            total_count = context_two[previous_two_words]\n",
    "        return np.log2(count/ total_count)\n",
    "    return np.log2(.8) + get_log_frequency_bigram(word = word[1:], \\\n",
    "                                                      context_one = context_one, context_two = context_two)\n",
    "\n",
    "\n",
    "def print_perplexity_scores_helper(n, log_scorer, dataset, n_gram_type, context):    \n",
    "    perplexity = lambda j: np.power(2, - np.mean([log_scorer(\n",
    "        context_one = context[1], context_two = context[2], context_three = context[3],\\\n",
    "        word = i) for i in j]))\n",
    "    perplexities_avg = sum(list(map(lambda x: perplexity(x), dataset)))/len(dataset)\n",
    "    print(f\"The average {n_gram_type} perplexity score of any test set tweet is : {perplexities_avg} \")\n",
    "    \n",
    "    \n",
    "def print_perplexity_scores():\n",
    "    global padded_amount\n",
    "    context = {1: {}, 2: {}, 3: {}}\n",
    "    funcs = {1: get_log_frequency, 2: get_log_frequency_bigram, 3: get_log_frequency_trigrams}\n",
    "    decs = {1: \"unigram\", 2: \"bigram\", 3: \"trigram\"}\n",
    "    for n in range(1, 4):             \n",
    "        train_set, test_set, model = gen_new_model(n)\n",
    "        context[n] = copy.deepcopy(model.vocab.counts)\n",
    "        if n == 2: \n",
    "            padded_amount = sum(Counter({k: c for k, c in model.vocab.counts.items() \\\n",
    "                                   if k[1] == '</s>'}).values()) # symmetrical with <s>\n",
    "        print_perplexity_scores_helper(n, funcs[n], test_set, decs[n], context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "5529f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram perplexity score of any test set tweet is : 670.2454773666304 \n",
      "The average bigram perplexity score of any test set tweet is : 58.82761504218191 \n",
      "The average trigram perplexity score of any test set tweet is : 35.109474179202245 \n"
     ]
    }
   ],
   "source": [
    "# NFTs\n",
    "train, test = start_generator(\"nfts.csv\")\n",
    "print_perplexity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9374cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram perplexity score of any test set tweet is : 916.0631700646361 \n",
      "The average bigram perplexity score of any test set tweet is : 282.4735987661044 \n",
      "The average trigram perplexity score of any test set tweet is : 233.4433476582671 \n"
     ]
    }
   ],
   "source": [
    "# Football\n",
    "_, test = start_generator(\"football.csv\")\n",
    "print_perplexity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "765ce651",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_, model = gen_new_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "7e5e5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('codes.json')\n",
    "emoji_codes = json.load(f)\n",
    "f.close()\n",
    "\n",
    "emoji_codes = {k: c for c,k in emoji_codes.items() }\n",
    "emoji_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58934d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram, just pick a random word \n",
    "\n",
    "# bigram, pick a random first word with <s> , then use the end of that word to generate the next\n",
    "# and so on until hitting a </s>\n",
    "\n",
    "#trigram, same idea, except a jump of 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27487f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def unigram_generator(length = 10):\n",
    "    string = ''\n",
    "    for i in range(10):\n",
    "        random_choice = random.choices(*zip(*train_vocab.items()))[0][0]\n",
    "        if rand_choice in emoji_codes:\n",
    "            rand_choice = emoji_codes[rand_choice]\n",
    "        elif random_choice == 'ethe_addy':\n",
    "            rand_choice = something\n",
    "        elif random_choice == 'dollar_amnt':\n",
    "            rand_choice = something\n",
    "        elif rand_choice = 'random_num'\n",
    "            rand_choice = something \n",
    "        elif rand_choice = 'randomhandle'\n",
    "            rand_choice = something\n",
    "            \n",
    "        string += rand_choice\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
