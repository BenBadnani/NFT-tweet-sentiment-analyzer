{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e965f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import re\n",
    "from nltk.lm import MLE\n",
    "import demoji \n",
    "from nltk.lm import Vocabulary\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da5941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_url(query, start_time, end_time, max_results= 100):\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    query_params = {\"query\": f\"{query} lang:en\", \"expansions\": \"geo.place_id\",\n",
    "                    \"max_results\": max_results, \n",
    "                    \"place.fields\": \"country_code,full_name,geo\",\n",
    "                    \"tweet.fields\": \"author_id\",\n",
    "                   \"next_token\": {}}\n",
    "    return (url, query_params)\n",
    "    \n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code) + \"\\n\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1617741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keyword = \"nft\"\n",
    "max_results = 100\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "url, query_params = create_url(keyword, start_time, end_time, max_results)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "203074e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"csvFile = open(\"nfts.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['author id', 'tweet id', 'text', 'geo', 'country_code', 'full_name', 'bbox'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ae188a13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n",
      "Endpoint Response Code: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"count = 0\n",
    "json_response = connect_to_endpoint(url, headers, query_params)\n",
    "amnt = 0\n",
    "\n",
    "while(count < 10000):\n",
    "    count += amnt\n",
    "    for step, tweet in enumerate(json_response['data']):\n",
    "        author_id = tweet['author_id']\n",
    "        tweet_id = tweet['id'] \n",
    "        text = tweet['text'] \n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "        try: \n",
    "            country_code = json_response['includes']['places'][step]['country_code']\n",
    "        except:\n",
    "            country_code = \" \"\n",
    "        try: \n",
    "            full_name = json_response['includes']['places'][step]['full_name']\n",
    "        except:\n",
    "            full_name = \" \"\n",
    "        try: \n",
    "            bbox = json_response['includes']['places'][step]['geo']\n",
    "        except:\n",
    "            bbox = \" \"\n",
    "\n",
    "        res = [author_id, tweet_id, text, geo, country_code, full_name, bbox]\n",
    "        csvWriter.writerow(res)\n",
    "    \n",
    "    amnt = json_response['meta']['result_count']\n",
    "        \n",
    "    json_response = connect_to_endpoint(url, headers, query_params, \n",
    "                                        next_token = json_response['meta']['next_token'])\n",
    "    \n",
    "\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fbe40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/25363498/is-there-a-standard-approach-to-returning-values-from-coroutine-endpoints\n",
    "class FINISH_PROCESSING_SIGNAL(Exception): pass\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "def tokenize_remove_emoticons(elem):\n",
    "    tokenized_list = tknzr.tokenize(elem)\n",
    "    # replacing emojis with their description, \n",
    "    # after tokenizer so as not to make them seperate words\n",
    "    return list(map(lambda x: demoji.replace_with_desc(x), tokenized_list))\n",
    "def tokenize_sentences(array_elem):\n",
    "    return list(map(lambda x: tokenize_remove_emoticons(x), array_elem))\n",
    "vf = np.vectorize(tokenize_sentences)\n",
    "\n",
    "def coroutine(func):\n",
    "    @functools.wraps(func)\n",
    "    def start(*args, **kwargs):\n",
    "        cr = func(*args, **kwargs)\n",
    "        next(cr)\n",
    "        return cr\n",
    "    return start\n",
    "\n",
    "def source(df, targets):\n",
    "    for target in targets: \n",
    "        target.send( df)\n",
    "        processed_data = target.throw(FINISH_PROCESSING_SIGNAL)\n",
    "        return processed_data\n",
    "    \n",
    "\"\"\"\n",
    "Note, these transformations are completely deterministic\n",
    "irrespective of the dataset being used, thus we can apply \n",
    "this initial part on the entire datafram.\"\"\"\n",
    "@coroutine  \n",
    "def clean_train_test_split(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            df = (yield)\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'0x[a-fA-F0-9]{40}', 'ethe_addy', x)) \n",
    "            #lowercasing all letters\n",
    "            df['text'] = df.text.str.lower()\n",
    "            # will keep the handles for the sake of n-gram approximation, but specifics don't matter\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"@([a-zA-Z0-9_]{1,50})\", 'randomhandle', x))\n",
    "            # removing url links\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "            # removing embedded links, videos\n",
    "            df.text = df.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "            df.text = df.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "            train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "            for target in targets: \n",
    "                target.send( (train, test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine  \n",
    "def train_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            trainer = PunktTrainer()\n",
    "            train, test = (yield)\n",
    "            array_strs =train[['text']].values.flatten()\n",
    "            array_strs_test =test[['text']].values.flatten()\n",
    "            map(lambda x: trainer.train(x), array_strs)\n",
    "            trainer.finalize_training()\n",
    "            tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "            for target in targets: \n",
    "                target.send( (tokenizer, array_strs, array_strs_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "            \n",
    "@coroutine  \n",
    "def segment_sentences(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            tokenizer, x, test = (yield)\n",
    "            segment_sentences = np.array([tokenizer.tokenize(xi) for xi in x], dtype = object)\n",
    "            segment_sentences_test = np.array([tokenizer.tokenize(xi) for xi in test], dtype = object)\n",
    "            for target in targets: \n",
    "                target.send( (segment_sentences, segment_sentences_test))\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "\n",
    "@coroutine   \n",
    "def sentence_tokenizer(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            x, test = (yield)\n",
    "            x, test = vf(x), vf(test)\n",
    "            flattened, flattened_test = [], []\n",
    "            for xi in x:\n",
    "                flattened.extend(xi)\n",
    "            for xi in test:\n",
    "                flattened_test.extend(xi)\n",
    "            for target in targets: \n",
    "                target.send( (flattened, flattened_test) )\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "            \n",
    "@coroutine  \n",
    "def train_model(targets):\n",
    "    try:\n",
    "        while True: \n",
    "            n = 3\n",
    "            training_set, test = (yield)\n",
    "            train, vocab = padded_everygram_pipeline(n, training_set)\n",
    "            model = MLE(n)\n",
    "            model.fit(train, vocab)\n",
    "            for target in targets: \n",
    "                target.send((model, training_set, test))\n",
    "    except FINISH_PROCESSING_SIGNAL as err:\n",
    "        yield target.throw(err)\n",
    "        \n",
    "@coroutine\n",
    "def sink():\n",
    "    data = {}\n",
    "    try:\n",
    "        while True:\n",
    "            data = (yield)\n",
    "    except FINISH_PROCESSING_SIGNAL:\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1eafd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2365: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  otypes = ''.join([asarray(outputs[_k]).dtype.char\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = sink()\n",
    "b = train_model(targets = [a])\n",
    "c = sentence_tokenizer(targets = [b])\n",
    "d = segment_sentences(targets = [c])\n",
    "e = train_tokenizer(targets = [d])\n",
    "f = clean_train_test_split(targets = [e])\n",
    "\n",
    "df = pd.read_csv(\"nfts.csv\")\n",
    "\n",
    "model, training_set, test_set = source(df, targets = [f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b65f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
