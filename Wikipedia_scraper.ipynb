{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5257b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter \n",
    "import copy\n",
    "from re import search\n",
    "import random \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "37ce7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = urlopen('https://en.wikipedia.org/wiki/Non-fungible_token').read()\n",
    "# Make a soup \n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "content_urls = [i for i in re.findall(r'<a href=([^\\s > # ]+)', str(soup.find_all('p')) ) if i != '\"']\n",
    "content_urls = [i.strip('\\\"') for i in content_urls]\n",
    "content_urls = ['https://en.wikipedia.org' + i for i in content_urls]\n",
    "\n",
    "f = open(\"wikipedia_articles.txt\", \"w\")\n",
    "\n",
    "for url in content_urls:\n",
    "    source = urlopen(url).read()\n",
    "    # Make a soup \n",
    "    soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "    # Extract the plain text content from paragraphs\n",
    "    text = ''\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text += paragraph.text\n",
    "\n",
    "    # dropping footnotes, removing newline chars\n",
    "    text = re.sub(r'\\[.*?\\]+', '', text).replace('\\n', '')\n",
    "    f.write(text + '\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "750fc675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c76caf19c94afab2bf9c1c7b785af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed = []\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "    \n",
    "    \n",
    "f = open(\"wikipedia_articles.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in tqdm(lines): \n",
    "    preprocessed.extend(nlp(line.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "59c4be9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 1140), ('game', 1036), ('work', 1009), ('include', 938), ('bitcoin', 786), ('use', 762), ('new', 752), ('release', 734), ('time', 661), ('blockchain', 588), ('copyright', 572), ('year', 562), ('$', 556), ('create', 553), ('company', 547), ('user', 494), ('right', 482), ('\\xa0', 471), ('art', 469), ('video', 451)]\n"
     ]
    }
   ],
   "source": [
    "non_stop_words_frequency = Counter([token.lemma_ for token in preprocessed if (not token.is_stop) and (not token.is_punct)])\n",
    "common_words = non_stop_words_frequency.most_common(20)\n",
    "print(common_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
